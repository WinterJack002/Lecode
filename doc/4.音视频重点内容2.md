# 基础知识

## 1.视频码率？

**视频码率（Bitrate）是指每秒传输或处理的比特数**，用于描述视频文件的大小和传输速度。视频码率是影响视频质量和文件大小的重要参数，通常以每秒比特数（bps, bits per second）或其更高的单位（如kbps、Mbps）表示。

**(1)**视频码率的分类

1. 恒定码率（CBR, Constant Bitrate）：

   在整个视频文件中，码率保持恒定。

   **优点：**易于预测文件大小和带宽需求，适用于实时流媒体传输。

   **缺点：**在复杂场景可能不够灵活，导致质量波动。

2. 可变码率（VBR, Variable Bitrate）：

   码率根据视频内容的复杂度动态变化，在简单场景下使用较低码率，在复杂场景下使用较高码率。

   **优点：**更高的编码效率和视频质量，适用于离线视频文件。

   **缺点：**文件大小和带宽需求难以预测，不适合实时传输。

3. 平均码率（ABR, Average Bitrate）：

   通过在视频中动态调整码率，保证整体码率接近设定的平均值。

   **优点：**兼顾了CBR和VBR的优点，适用于需要平衡质量和文件大小的场景。

   **缺点：**复杂度较高，可能需要更多的计算资源。

**(2)**视频码率的单位

1. bps（Bits per Second）：**每秒传输的比特数。**
2. **kbps（Kilobits per Second）：**每秒传输的千比特数，1 kbps = 1000 bps。
3. Mbps（Megabits per Second）：**每秒传输的兆比特数，1 Mbps = 1000 kbps。

**(3)**视频码率与视频质量

**高码率：**提供更高的视频质量，因为更多的比特可以用于表示视频细节。文件大小更大，传输和存储要求更高。适用于高分辨率、高帧率的视频，如4K视频和专业视频制作。

**低码率：**视频**质量较低**，可能会出现压缩伪影（如马赛克、模糊）。文件大小更小，传输和存储要求更低。适用于带宽有限的场景，如移动设备、低速网络环境

**(4)**视频码率的选择

高分辨率（如1080p、4K）和高帧率（如60fps）的视频通常需要更高的码率。

低分辨率（如480p、720p）和低帧率（如24fps、30fps）视频可以使用较低的码率。

## 2.视频质量怎么评估，码率高就一定质量好吗？

**分辨率**：分辨率决定了视频的清晰度和细节表现

**帧率**：帧率越高，视频的流畅性越好，**运动物体的显示越清晰**

**色彩深度**：色深越高，视频的颜色表现越准确，过渡越平滑，尤其是在渐变和阴影部分表现更为明显。低色深的视频可能出现**色带现象**（banding），即颜色过渡不够平滑。

**编码效率**(不同的编码器)。

码率是一个重要的衡量标准，但它不是决定视频质量的唯一因素。在相同的分辨率、帧率、色彩深度、编码器下，码率越高，视频质量越好。

# 编解码

## 0.H264中的SVC是什么？

在H.264视频编码标准中，**SVC**指的是**Scalable Video Coding**（可分级视频编码），它是H.264标准的一个扩展，旨在增强视频流的灵活性和适应性。SVC允许视频流具有多种分辨率、帧率和质量级别，从而使同一个视频流能够适应不同的网络带宽和设备处理能力。

### SVC的核心思想

SVC的核心在于将视频分为**多个层次**，每一层次代表视频的不同级别信息，包括**分辨率**、**帧率**和**质量**等。这种分级结构使得客户端可以根据网络条件和设备性能选择合适的层次进行解码，从而平衡视频质量和资源消耗。

SVC主要包括以下几种分级：

1. **空间分级（Spatial Scalability）**：
   - 视频流中包含不同分辨率的层次。例如，可以同时包含360p、720p和1080p等分辨率。
   - 用户设备可以根据屏幕分辨率和带宽选择合适的分辨率层次进行解码，节省带宽。
2. **时间分级（Temporal Scalability）**：
   - 视频流中包含不同帧率的层次。例如，可以同时包含15fps、30fps和60fps等帧率。
   - 客户端可以选择较低的帧率层次，从而在网络条件较差时减少数据传输量。
3. **质量分级（Quality Scalability）**：
   - 视频流中包含不同质量的层次（即不同的比特率），如标准清晰度和高清晰度。
   - 当带宽足够时，客户端可以选择高质量层次，而在带宽不足时可以选择低质量层次。

### SVC 的优点

1. **适应性强**：SVC能够根据带宽自动调整视频质量，适合多种网络环境（如3G、Wi-Fi等）。
2. **节省带宽**：SVC允许客户端只解码特定层次，减少数据传输量和解码复杂度。
3. **适用于多设备**：SVC适合具有不同分辨率、处理能力和带宽的设备，实现一次编码、多端适用。
4. **流畅切换**：在视频会议、直播等场景中，SVC支持流畅切换不同清晰度和帧率，有效避免了网络波动带来的卡顿。

### 应用场景

SVC在视频会议、视频流媒体传输、低带宽网络中的视频播放等场景中应用广泛。特别是在视频会议中，SVC使得服务端可以提供不同的分辨率和帧率，让用户在不同的网络条件下获得较好的体验。

## 1. H.265与H.264在编码上的差别

1. **编码块结构**

   H.264：使用16x16像素的宏块作为编码的基本单位。

   H.265：引入了编码树单元（CTU），取代了宏块，CTU的大小可以是16x16、32x32甚至64x64像素。CTU可以进一步分割为更小的编码块（CB）、预测块（PB）和变换块（TB），且分割方式更灵活，支持不规则分割，增强了对复杂场景的适应性。

2. 帧内预测

   H.264：16x16的亮度块，4种预测模式；4x4的亮度快提供9种预测模式，色度块是4种预测模式。

   H.265：亮度分量支持5种大小的预测单元PU：4x4、8x8、16x16、32x32、64x64。每种PU都对应了35种预测模式，包括Planar模式、DC模式以及33种角度模式。

3. 帧间预测

   H.264：支持多参考帧、四分之一像素精度的运动估计和双向预测。

   H.265：进一步提高运动估计的精度，支持更高的运动向量精度（如八分之一像素），并引入了高级运动矢量预测（AMVP）和合并模式（Merge Mode），优化了运动补偿的效率。

4. 变换与量化

   H.264：使用4x4和8x8的整数离散余弦变换进行变换，量化方式相对简单。

   H.265：使用32bit精度的浮点离散余弦/正弦变换变换，可以自适应选择DCT或DST。变换块大小包括4x4、8x8、16x16和32x32，提高 高分辨率视频的压缩率。

5. 熵编码

   H.264：采用CAVLC(自适应变长编码)和CABAC(自适应二进制算术编码)两种熵编码方式，其中CABAC在高码率场景下效率更高。

   H.265：使用基于二进制树的CABAC(基于算术编码)，提高了编码效率和压缩比。提高上下文建模的复杂度。针对并行处理做了优化。

6. 去块滤波与样条滤波

   H.264：采用去块滤波器减少块效应，并使用**自适应变换跳变滤波器**（SAO）来减少伪影。
   H.265：H.265引入了样条滤波器，用于进一步减少压缩噪声，提高视觉质量。

7. 编码效率与复杂度

   编码效率：在同等视频质量下，H.265的压缩效率比H.264高约50%

   编码复杂度：H.265的编码复杂度显著高于H.264。

## 2. H.264有哪些关键技术？

1. 分块与分区

   1. 宏块：视频帧被分割成16x16像素的宏块，每个宏块包含亮度和色度信息。
   2. 子块和块分割：宏块可以进一步分割为更小的块（如8x8、4x4等）以更精细地捕捉运动信息。

2. 帧内预测：H.264支持多种帧内预测模式，不依赖其他帧，通过空间相关性减少编码残差。

   支持4x4、8x8和16x16块大小的预测，提升预测精度。亮度三种都有，前两种有九种预测模式；16x16有4种。色度只有8*8的4种预测模式。(如DC预测、水平预测、垂直预测）

3. 帧间预测：**利用相邻帧之间的相似性**来预测当前帧的像素值，从而减少冗余数据。

   1. 多参考帧预测：支持多个参考帧（最多16个），提高运动补偿精度。
   2. 分块运动补偿：支持不同大小块的运动估计。
   3. 支持四分之一像素精度的运动矢量，提高预测效果。
   4. 利用前后参考帧共同预测当前帧，增强双向运动表达。

4. 变换与量化：

   1. 整数变换：使用4x4或8x8的整数DCT，避免舍入误差。、
   2. 量化：减少数据的精度以提高压缩率。动态调整量化步长和量化矩阵，提高压缩效率。

5. 熵编码：

   1. CAVLC：基于变长编码，适用于低码流场景。
   2. CABAC：基于算术编码，提供更高压缩率，适用于高码率场景。

6. 去块滤波

   在解码后对宏块进行去块滤波，减少块效应，提升视频质量。H.264引入在线循环滤波器(减少块效应)和多帧平滑滤波器(改善视频流的平滑性)。

7. 网络抽象层

   1. NAL将编码数据封装成适合网络传输的格式。
   2. NAL单元包含视频参数集（SPS）、图像参数集（PPS）、视频片段（slice）、补充增强信息等数据。1080P的视频帧，大约划分成4-8个NAL单元。
   3. NAL 单元由两部分组成:
      - NAL 单元头:1 字节的头部信息,用于标识 NAL 单元的类型。
      - NAL 单元有效载荷：可变长度的实际数据内容。

8. 自适应帧场编码

   根据视频内容的运动特性，自适应选择帧编码或场编码，提高压缩效率和图像质量。场编码将每帧分成两个场独立编码(奇偶场)，适用于运动较大的场景。

## 3. 为什么要对音视频编解码？

通过对音频和视频信号的压缩和解压缩，实现高效存储和传输。

1. 减少数据量
   1. 提高传输效率
   2. 提高存储效率
2. 兼容性和标准化
   1. 提供了标准化的音视频格式，使得不同设备和应用能够互相兼容

## 4. 软编码与硬编码的优缺点

根据编码的实现方式，可以分为**软件编码（软编码）和硬件编码（硬编码）**

1. 软编码（Software Encoding）

   软编码是指通过**软件算法对音视频数据进行编码的方式**。软编码依赖于编解码库和软件实现的算法，例如FFmpeg、x264等。

   优点：

   1. 灵活性高：软件编码可以方便地调整编码参数；易于更新和维护：可以快速迭代和优化编码算法
   2. 成本较低：无需额外的硬件设备，降低了设备成本。

   缺点：

   1. 性能有限：编码速度和效率受到CPU性能的限制
   2. 功耗较高：软编码需要大量计算，导致高功耗，特别是在移动设备上，可能会加速电池耗尽。
   
2. 硬编码（Hardware Encoding）

   硬编码是指利用专用的硬件编码器对音视频数据进行编码。

   **优点：**

   1. 高性能：硬编码器设计专用于音视频编码，效率和速度更快。
   2. 低功耗：硬编码器比软编码更节能，因其在功耗和性能上更优，特别适合嵌入式和移动设备，延长电池续航。

   缺点：

   1. 灵活性较低：硬编码器的编码参数和功能较为固定，难以灵活调整和定制，不如软编码灵活。
   2. 成本较高：需要额外硬件，增加成本和复杂度，无法快速更新。
   3. 开发难度大：开发硬件编码器技术门槛高，涉及硬件设计和低级编程，周期长，且难以快速实现算法创新和优化。


## 5. H264的编解码过程+AAC的解码过程

音视频解码器是**将压缩编码的音视频数据还原为可以播放的原始数据**的工具。

### 1.H264编码

1. 帧获取和分析：H.264 编码器获取原始的未压缩视频帧，将帧分类为**I 帧**（关键帧）、**P 帧**（预测帧）和**B 帧**（双向预测帧）。

2. 在编码 I 帧时，H.264 使用**帧内预测**技术。帧内预测通过当前帧内的相邻像素来预测当前块的像素值，减少数据冗余。再得到残差帧.

   左上角一般使用DC预测模式，使用全局平均值预测当前像素值，一般使用固定值128进行DC预测。

   第一行，第一列的子块可以通过垂直、水平预测。其他块可以使用平面预测、对角线预测。

   在H265和H266(VVC：90多种)中，预测的方向也更加多元化，

3. 对于 P 帧和 B 帧，H.264 使用**帧间预测**。帧间预测基于其他参考帧来预测当前帧，以减少数据冗余。当前帧与参考帧的运动向量得到预测帧，然后得到残差帧。

4. 对残差帧进行DCT变换(DCT 将图像的能量集中在低频部分，便于压缩。)和量化（对DCT系数进行缩放和舍入，有量化矩阵）。

5. 熵编码：上下文自适应可变长编码、上下文自适应 二进制算术编码。压缩成紧凑的比特流，更高效的存储或传输。

6. NAL单元封装：将编码后的数据封装成NAL单元，便于网络传输。

### 2.H.264 解码

H.264 是一种常见的视频压缩标准，用于高清数字视频的压缩。H.264 解码器的工作流程如下：

1. 读取码流：**解码器从输入文件或流中读取 H.264 编码的压缩码流。**
2. **分离NAL单元：**将码流分割成网络抽象层（NAL）单元，每个 NAL 单元包含一个编码的视频帧或帧片段。
3. 解析头信息：**解析序列参数集（SPS）和图像参数集（PPS）等头信息，以获取视**频的基本属性（如分辨率、帧率、色彩格式等）。
4. **熵解码：**对码流进行熵解码（如 CAVLC 或 CABAC），将压缩数据转换为量化后的变换系数。(上下文自适应可变长编码、上下文自适应二进制算术编码)
5. 反量化：**对量化后的变换系数进行反量化，恢复到变换前的状态。**
6. **逆变换：**对反量化后的数据进行逆变换（如逆离散余弦变换），得到残差帧。
7. **帧间**/帧内预测重建：使用**预测帧**(参考帧+运动向量)和**残差帧**重建视频帧。
8. **去块滤波：**对重建后的图像进行去块滤波，减少块效应。
9. 输出视频帧：将解码后的视频帧输出到显示设备或进一步处理。

### 3.AAC 解码器

AAC（Advanced Audio Coding）是一种广泛使用的音频编码格式。AAC 解码器的工作流程如下：

1. 读取码流：**解码器从输入文件或流中读取 AAC 编码的压缩码流。**
2. **解析ADTS/ADIF头：**解析音频数据传输流（ADTS）或音频数据交换格式（ADIF）的头信息，以获取音频的基本属性（如采样率、声道数等）。
3. 熵解码：**对码流进行熵解码（如霍夫曼编码），将压缩数据转换为量化后的频域系数。**
4. **反量化：**对频域系数进行反量化，恢复到量化前的状态。
5. 逆变换：**对反量化后的频域系数进行逆变换（如逆MDCT变换），得到时域音频信号。**
6. **音频重建：**对时域信号进行音频重建，处理声道混合、增益调整等。
7. 输出音频样本：将解码后的音频样本输出到音频设备或进一步处理。

## 5.1 H264/H265为什么会有块效应？

1. 压缩时，每个块独立处理，块之间出现不连续，导致边界明显。
2. 块估计算法中，一个子块有一个运动向量，块与块之间的运动矢量差异较大时，运动补偿后这些块之间的边界会非常明显，从而加剧块效应。

## 6. 什么是I帧 B帧 P帧？GOP画面组是什么？

1. I帧--->**使用帧内压缩**
   定义： I帧是独立编码的帧，不依赖其他帧的数据。

   1. 独立性：I帧包含完整的图像数据，类似静态图像，可独立解码和显示。
   2. 大小较大： 由于包含完整图像，I帧数据量较大，压缩率低。
   3. 关键帧： I帧作为关键帧，用于快速定位和随机访问，便于视频播放和编辑。

2. IDR帧：特殊的I帧，强制解码器清空参考帧的缓存，保证在IDR帧之后的P帧和B帧不能引用IDR帧之前的帧。用于切断前后帧的引用关系，具有更强的解码刷新功能，在IDR帧处可以完全重新同步。流媒体播放器在网络不稳定或丢包时，常通过 IDR 帧重新同步视频流。

3. P帧--->使用帧间压缩

   定义： P帧（前向预测帧）基于前面的I帧或P帧进行编码。

   1. 依赖前帧：P帧参考前面的I帧或P帧，仅记录变化部分的数据。
   2. 压缩率高：数据量小，压缩率高。
   3. 依赖性：解码P帧时，需依赖参考帧（I帧或前面的P帧）。

4. B帧--->使用帧间压缩

   定义：B帧，双向预测帧，基于前面的I帧或P帧和后面的P帧或I帧进行编码。

   1. 双向预测：B帧通过参考前后的帧，记录双向预测差异信息，只包含变化部分的数据。
   2. 最高压缩率：B帧的压缩效率最高，数据量最小。
   3. 依赖性强：解码B帧时，需要依赖参考帧（前后的I帧或P帧）
   4. 复杂性高：编码和解码B帧时需要处理双向预测，计算复杂度较高。

5. GOP

   定义：GOP是由一系列连续的I帧、P帧和B帧组成的图像组，是视频编码中的基本单元。

   由一个I帧开始，后续跟随多个P帧和B帧。

   1. GOP长度：I帧之间的距离。
   2. 固定GOP：GOP长度恒定，I帧间隔固定。
   3. 自适应GOP：根据场景变化动态调整I帧间隔。 

## 7.什么是SPS和PPS？区别是什么？

1. **SPS**（序列参数集）
   1. 包含的参数：视频分辨率、帧率、视频编码配置文件和级别、色彩空间等。
   2. **这些参数对整个视频序列有效，**变化不频繁。提供视频解码的基础框架信息。
2. **PPS**（图像参数集)
   1. **包含对特定图像组或单个图像的**局部编码设置
   2. 包含的参数：量化参数、熵编码模式（如CABAC或CAVLC）、切片分割信息等
   3. 影响范围：**主要影响序列中的特定帧或帧组，参数可以根据图像内容的不同而**频繁变化.解析PPS依赖于SPS提供的信息。

## 8.h264中的AVCC和Annex B

AVCC 是包含在容器（如 MP4、FLV 等）中的配置信息，用来初始化 H.264 解码器。它包含一些关于视频编码的元数据，这些信息是解码 H.264 码流所必需的，特别是在 MP4 或 FLV 等封装格式中，视频数据流往往以分段或片段的形式保存。因此，在解码器开始处理实际的视频帧之前，AVCC 提供了解码器所需的参数。

视频数据可以通过两种方式封装：

1. Annex B

   **Annex B 格式**：每个 NAL（Network Abstraction Layer）单元前有一个 `0x000001` 或 `0x00000001` 的开始码。这种方式通常用于裸 H.264 码流，适合流媒体传输场景。

2. AVCC

   **AVCC**（AVC Decoder Configuration Record）是 H.264 编码流中用于初始化解码器的关键结构，它包含了流的 **Profile**、**Level**、**SPS** 和 **PPS** 等重要信息。每个NAL单元前边都有一个NAL单元长度片段。

   AVCC 中包含的关键信息包括：

   1. **版本号**：通常为 1。
   2. **Profile、Compatibility 和 Level**：这些字段描述了 H.264 码流使用的编码特性。例如，Profile 可以是 Baseline、Main、High 等，代表了使用什么编码技术，比如B帧的使用、熵编码的选择，而 Level 则表示复杂度级别，决定了分辨率和帧率。
   3. **SPS（Sequence Parameter Set）**：序列参数集，描述了视频帧的全局参数，如帧的宽度、高度、帧率等。
   4. **PPS（Picture Parameter Set）**：图像参数集，描述了具体图像块的编码参数。
   5. **NAL 单元长度字段**：指定每个 NAL 单元的长度信息，解码器可以使用这些长度信息来正确处理 H.264 NAL 单元。

**AVCC 更常用于存储和文件格式**：如 MP4、MOV 中，因为这种格式可以直接在文件开头指定 SPS 和 PPS 信息，避免在每个帧中重复携带，提高了编码效率。

**Annex B 更常用于实时流媒体传输**：如网络视频流（RTSP、RTP）以及 MPEG-TS 传输流格式等，因为 `start code prefix` 提供了流的边界标识，便于实时传输和解码。

## 9.I帧和P帧的判断？

通过NAL单元来判断。
NAL 单元由两部分组成:
- NAL 单元头:1 字节的头部信息,用于标识 NAL 单元的类型。
- NAL 单元有效载荷：可变长度的实际数据内容。

NAL单元，有SPS、PPS类型，非IDR(P、B帧)、IDR片段(通常是I帧)、SEI(补充增强信息)。

 forbidden_zero_bit (1 bit) | nal_ref_idc (2 bits) | nal_unit_type (5 bits) |
①通常必须为0，非0表示错误。
②优先级：00最低
③NAL单元类型，1-31，比如I帧(5)、P帧(1)、SPS等

## 10.H264编解码

编码分为：**视频编码层**和**网络抽象层**。**VCL** 是负责视频数据压缩的部分，它定义了如何对原始视频帧进行编码。**NAL** 是负责将 VCL 生成的压缩视频数据进行封装，以适应不同的传输和存储需求。

VCL 和 NAL 层的设计分离，使得 H.264 能够轻松适应多种不同的传输协议和应用场景

网络抽象层NAL：

1. NAL将编码数据封装成适合网络传输的格式。
2. NAL单元包含视频参数集（SPS）、图像参数集（PPS）、视频片段（slice）、补充增强信息等数据。1080P的视频帧，大约划分成4-8个NAL单元。
3. NAL 单元由两部分组成:
   - NAL 单元头:1 字节的头部信息,用于标识 NAL 单元的类型。
   - NAL 单元有效载荷：可变长度的实际数据内容。


 # 同步与传输

## 1. 音视频同步策略？

1. 基于时间戳的同步策略

   通过**使用音频和视频帧的时间戳（PTS）来实现同步**。时间戳表示帧的展示时间，播放器维护一个全局时钟，与音视频帧的时间戳对比，决定播放时机。

2. 主从时钟同步策略

   通过指定一个媒体流（通常是音频或视频）作为**主时钟**，另一个媒体流作为**从时钟**进行同步。

   1. **音频主时钟：**音频时间作为基准，视频根据音频的时间戳进行调整。这种策略适用于大多数场景，因为**人耳对音频延迟更敏感**。
   2. **视频主时钟：**视频时间作为基准，音频根据视频的时间戳进行调整。如视频会议。
   3. **实现方式**：主时钟的帧按时间戳播放，从时钟的帧根据主时钟进行调整。播放器通过对比主从时钟的时间戳，决定是加速还是延迟从时钟的帧。

3. 缓冲区管理策略

   通过合理设置缓冲区大小和管理策略，减少播放卡顿和延迟，实现音视频同步。

   1. **缓冲区大小设置**：根据网络状况和播放需求调整缓冲区大小，确保平稳播放。
   2. **缓冲区填充策略：**播放前先缓冲一段数据，减少播放中的卡顿。
   3. **实现方式**：播放器预先缓冲音视频数据，动态调整缓冲区大小以应对网络波动。

## 2.音视频同步的常见问题与解决方法？

**(1)**常见问题

1. 音视频起始点不同步

   音频和视频流在开始播放时不同步，主要原因是音频和视频数据在传输和处理过程中存在延迟差异。

2. 播放过程中音视频不同步

   在播放过程中，音视频逐渐不同步，音频可能比视频快或慢。

   1. 缓冲区大小设置不当
   2. 音频和视频的解码和渲染时间不同步
   3. 或者由于**网络抖动**和**数据包丢失导致**的数据延迟差异

3. 关键帧处理不当

   关键帧处理不及时，或者在解码过程中关键帧丢失或损坏，导致视频播放中断或卡顿，进而影响音视频同步。

**(2)**解决方法

1. 使用时间戳进行同步

   根据时间戳信息调整音频和视频的播放时间，使其同步。

2. 使用音视频同步算法

   通过监测音视频播放的时间差异，实时调整播放速度以保持同步。

3. 定期重新同步

   在播放过程中，**定期检查音视频的同步状态**，并根据需要进行重新同步调整。

4. 调整缓冲区大小

   在网络状况较差时增加缓冲区大小，确保播放流畅；在网络状况较好时减少缓冲区大小，降低播放延迟。

5. 使用硬件解码

   利用硬件解码器处理音视频数据，减轻CPU负担，提高解码和渲染速度。

6. 实时网络传输优化

   在弱网环境下，自适应调整音视频流的码率；使用**前向纠错技术修复丢失或损坏的数据包**，减少音视频不同步的发生。

## 3. PTS与DTS的区别？

| **比较点**   | **PTS**                        | **DTS**                         |
| ------------ | ------------------------------ | ------------------------------- |
| **定义**     | 显示时间戳                     | 解码时间戳                      |
| **主要作用** | 确定帧的显示时间               | 确定帧的解码时间                |
| **用途**     | 音视频同步，控制帧的播放顺序   | 控制解码顺序，处理B帧时尤其重要 |
| **产生方式** | 通常在封装过程中生成           | 通常在编码过程中生成            |
| **适用场景** | 音视频播放，确保同步和顺序播放 | 视频解码，确保解码顺序          |

## 4.如何评估音视频传输质量，在弱网环境下如何确保传输质量？

**(1)评估音视频传输质量的指标**

1. **延迟：**延迟是指数据从源端传输到目的端所需的时间，通常以毫秒（ms）为单位。高延迟会导致音视频不同步、对话延迟等问题。
2. **抖动**：抖动是指数据包到达时间的变化，通常以毫秒（ms）为单位。高抖动会导致音视频播放不平滑，产生卡顿现象。
3. **丢包率：**丢包率是指在传输过程中丢失的数据包比例，通常以百分比表示。高丢包率会导致音视频信号不完整，产生马赛克、音频缺失等问题。

**(2)弱网环境下的优化策略**

1. **数据压缩**

   对音视频数据进行有效压缩，以减少传输数据量。在有限带宽条件下传输高质量音视频，减少延迟。

2. **带宽预测**

   使用机器学习算法分析带宽历史数据，预测未来的带宽变化趋势。根据预测结果动态调整码率和缓冲策略。

3. **自适应比特率**

   使用 HLS、DASH 等协议实时监测带宽，根据网络变化切换不同码率的视频流，客户端选择最佳质量的流媒体切片，降低延迟。在不同网络条件下保持流畅播放，减少卡顿。

4. **缓冲区管理**

   在网络状况较差时增加缓冲区大小，以平滑播放；在网络状况较好时减少缓冲区大小，以降低延迟。动态调整缓冲策略，确保在弱网环境下提供足够的缓冲时间。提高视频播放的连续性和流畅性，减少播放中断。

5. **前向纠错（Forward Error Correction, FEC）**

   在发送数据时增加冗余信息，以便在接收端进行错误恢复。使用RS编解码算法将冗余数据嵌入音视频流中。接收端通过冗余数据进行丢包恢复，提高传输可靠性。减少丢包。

6. **重传机制**

   1. 概述：在检测到数据包丢失时，发送端重新发送丢失的数据包。
   2. 实现方式：使用RTCP协议中的NACK机制，接收端请求发送端重传丢失的数据包。重传机制确保关键数据不会丢失。

7. **协议优化**

   使用高效、低延迟的传输协议来减少传输延迟和丢包率。采用优化协议（如QUIC、SRT）替代传统TCP/UDP，提升传输效率和稳定性，具备更好的拥塞控制和丢包恢复能力。

## 5.H264为啥要用YUV颜色空间?

1. 人眼感知特性与压缩效率高
人眼对亮度信息更敏感，而对色度信息较不敏感。YUV 颜色空间将图像的亮度和色度分离开来，使得压缩过程中可以对色度信息进行更高比例的压缩，而不显著降低视觉质量。比如YUV420P格式，四个Y共享一对UV。在保持足够图像质量的同时大大减少了数据量。
2. 编码优化
H.264 编码器对 YUV 格式进行了优化，尤其是在图像的块状编码、运动估计等步骤中，YUV 空间更利于这些算法的高效处理。
3. 计算复杂度与存储传输优化
   YUV 颜色空间减少了不必要的颜色冗余信息，这使得在压缩过程中编码器的处理复杂度降低。例如，色度分量的降低减少了运算量和存储带宽。提高了处理效率。
   在同样的分辨率下，使用 YUV 格式还可以减少存储提高传输效率。
4. 兼容性
YUV 颜色空间在早期的电视广播和视频处理标准（如 PAL、NTSC）中就被广泛使用。为了确保兼容性和适应现有的硬件，YUV 继续在数字视频编码标准（如 H.264）中占据主导地位。
国际标准：YUV 作为视频压缩和传输的国际标准（如 ITU-R BT.601 和 BT.709），被广泛应用于各类视频系统，如电视、流媒体、广播等，H.264 作为视频编码标准自然继承了这种惯例。
## 6.倍速播放的时候怎么处理音频才能让音频不变调

在倍速播放时，要让音频不变调，常用的技术是**时间伸缩（Time Stretching）**。这种技术可以改变音频的播放速度（即时间上的长度），而不改变音频的音高（Pitch）

1. **时间伸缩算法**
时间伸缩是一种通过改变音频信号的时间长度，而不改变其频率内容的处理方式。这意味着音频的节奏加快或减慢，但音调保持不变。常用的时间伸缩算法包括：
Phase Vocoder（相位声码器）：通过频域处理，保持相位连续性，以改变音频的时间长度，但保持音高不变。这是最常用的高质量时间伸缩算法之一。
SOLA（Synchronous Overlap and Add）：这种算法通过在音频片段之间增加或减少重叠区域来调整播放速度，但保持音高不变。SOLA 适用于中等到高质量的实时时间伸缩。()

2. Pitch Shifting（音高调整）

   倍速播放时，如果直接改变播放速度，音频的音高也会随之变化。例如，播放速度加快，音高也会上升。因此，使用时间伸缩技术时，必须避免音高随时间变化。这是**通过频域或时域算法将原始音高保留或进行音高补偿。**

3. 具体处理方法

   对于倍速播放，处理音频的流程如下：

   1. 倍速播放音频：首先通过改变音频的播放速度加快或减慢音频流，例如将1倍速变为1.5倍速或2倍速。
   2. 时间伸缩算法应用：应用时间伸缩算法来调整音频播放时长。此过程会改变音频的播放速率，但不改变音高。具体步骤包括：
      - 分析音频的频率和相位信息。
      - 调整时域上的片段，使其时间长度发生变化。
      - 保持相位的一致性，从而避免音高的变化。
   3. 音频再同步：调整后的音频需要与视频重新同步，确保在倍速播放下音视频匹配。

4. 实时处理
    在实际的播放器或流媒体应用中，倍速播放时的音频处理通常是**实时完成**的。现代播放器或音频处理库（如 FFmpeg、SoX、Rubber Band 等）已经**集成了高效的时间伸缩算法**，能够在用户选择倍速播放时，实时调整音频的播放速率而保持音高不变。
    虽然时间伸缩算法可以有效地保持音高不变，但过大的倍速（如 3 倍、4 倍）播放可能会导致音频质量的下降。因此，在处理时还需要考虑音质的平衡。某些高级算法可以通过增加计算量来提高音频质量，避免明显的失真和音频伪影。

## 7.弱网环境如何确保稳定播放？

1. 自适应码率流媒体（ABR，Adaptive Bitrate Streaming）

   原理：根据网络带宽动态调整视频的码率。当网络状况较差时，降低视频的分辨率、帧率和码率以保证视频流畅播放；当网络条件改善时，自动切换到更高质量的码率。
   实现方式：在服务器端准备不同码率的视频流（如 1080p、720p、480p 等），播放器通过监测网络带宽实时选择最合适的码率进行播放。
   协议支持：常见的流媒体协议如 HLS（HTTP Live Streaming）和 DASH（Dynamic Adaptive Streaming over HTTP）都支持自适应码率流媒体。

2. 合理的缓冲策略

   扩大缓冲区：在网络较差的情况下，播放器可以扩大初始缓冲区，提前加载更多的视频数据。这可以减少网络波动导致的频繁卡顿。
   **智能缓冲**：动态调整缓冲区大小，弱网条件下增加缓冲时间，网络稳定时减少缓冲时间，以减少延迟。
   **预加载**：在播放过程中，播放器可以在后台持续加载接下来的数据段，确保播放过程不会因数据加载不及时而中断。

3. 多 CDN 加速与边缘服务器缓存

   CDN（内容分发网络）加速：通过在全球范围内部署内容分发节点，尽可能将视频内容放在离用户最近的服务器上，减少延迟并提高数据传输速度。
   边缘服务器缓存：将视频内容缓存在离用户较近的边缘服务器，以便用户能从附近节点获取视频数据，避免因网络问题导致长时间的数据传输。

4. 使用低延迟视频传输协议

   WebRTC：WebRTC 是一种适合实时通信的协议，能够在弱网环境下通过 P2P 传输提供低延迟的视频流，适合用于视频通话或实时互动场景。
   RTMP：RTMP 可以用于低延迟的流媒体传输，尤其在大规模直播场景中有较好的表现。
   QUIC（Quick UDP Internet Connections）：QUIC 是一种基于 UDP 的新一代传输协议，能更好地适应网络波动，减少连接时间和重传延迟，适合实时视频流。

5. 使用更加高效的视频编码格式

   选择高效编码格式：使用像 H.265（HEVC）或 AV1 等现代视频编码格式，可以在保持相同画质的情况下大幅减少视频的码率。这在带宽有限的网络条件下，可以有效提高视频的传输效率。
   动态码率调整：编码器可以根据网络状况动态调整编码参数，在码率较低的情况下通过更多的压缩来减少数据量，但尽可能保持视频质量。

6. 错误恢复机制

   包丢失重传（ARQ, Automatic Repeat reQuest）：当检测到网络传输过程中丢包时，播放器可以请求服务器重发丢失的数据包。
   前向纠错（FEC, Forward Error Correction）：在传输时加入冗余数据，允许播放器在丢失部分数据的情况下仍然可以通过冗余数据重构完整的视频帧，减少因丢包导致的卡顿和视频质量下降。
   快速切换机制：当播放器检测到网络出现严重问题时，可以快速切换到备用 CDN 或服务器，避免长时间卡顿。

7. 动态丢帧

   动态丢帧：在弱网条件下，播放器可以选择智能丢弃一些帧，特别是在播放高帧率视频时，适当减少帧率以减少带宽需求，并且通过丢帧来保持播放流畅性，避免停顿。

8. 缩短关键帧间隔
   缩短 GOP（Group of Pictures）的长度，使得每个 I 帧（关键帧）之间的间隔更短，这样在丢包或重新加载时可以更快地恢复播放，而不需要等待下一个关键帧。

9. 用户提示和播放策略

   播放策略：当检测到弱网条件时，可以在用户端提供提示，告知用户网络问题，或者允许用户手动选择降低画质的选项。

# 格式与协议

视频封装：MP4、TS、fMP4、FLV

## FLV 

FLV的文件头有 音频标记类型、视频标记类型和版本号；
FLV的文件内容以FLVTAG形式存在，每个TAG独立。TAG=Header+Body。

Header: TAG类型(Audio、Video、脚本数据ScriptData)、数据大小(data大小)、时间戳(展示时间)、流id一直是0。11字节。
Body就是Data

TAG的排列方式是:上一个TAG的大小、TAG、上一个TAG的大小、TAG......

## MP4 + fMP4

MP4是最常见的对媒体文件格式。MP4文件主要分为两个部分，元数据(moov)和音视频数据(mdat)

MP4文件由许多的Box和fullbox(**FullBox** 是 **Box** 的一种特殊形式，它比普通的 Box 多了**版本号**和**标志位**。比如电影头部 Box：`mvhd`)组成，Box由Header和Data组成，Header(8字节)包含了Box的**长度**和**类型。**当一个Box的Data是一系列子Box时，称为容器。moov在mdat之前：MP4文件可以快速打开；moov放在mdat之后，需要下载完成才能进行播放。

**为什么需要fMP4：**①分段，实现自适应比特率流（ABR）②低延迟：通过小片段，可以更快地开始播放，减少用户等待时间③**错误恢复**：fMP4 允许在网络波动的情况下更容易地恢复播放，即使某些片段丢失，也可以从最近的有效片段继续。

![image-20240418163737956](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012014231.png)

fMP4是初始化段+媒体段组成。初始化段：ftyp+moov，类似MP4文件的开头，不同的是moov里边有mvex box，用来判定这是一个fMP4；mvex box中包含了指向各个片段的描述信息，允许播放器根据需要访问和播放特定片段；还包含trex，用于指示每个媒体轨道在片段中的特性，比如持续时间、时间戳。切片文件由moof+mdat box组成。moof中包含了mfhd(指定文件的持续时间，对应最长的轨道持续时间)和traf。

fMP4是一种封装格式，TS也是一种封装格式，TS全名是MPEG-TS

fMP4和HLS的区别？

fMP4是一种封装格式，HLS是一种流媒体传输协议。
在流媒体传输中，fMP4可以只包含一个片段，更适合低延迟流媒体场景，一般是2-4秒的片段，片段内部有元数据信息，可以快速解码。

## TS（MPEG-TS）

单独的流构成基本数据流(ES)->将ES按访问单元拆分得到分组的ES(PES)->切割成固定大小(184字节)。

PES包含ES+头(流标识符、包长度、时间戳); 
TS是184字节的PES + 4字节的头(包标识符、传输优先级(1位))。
TS文件：面向传输，流被分割成小块。
默认的分片时长通常是 6 秒。延迟在15-30s，低延迟的HLS是2-3S。

## fMP4相对TS文件的区别？

1. fMP4封装效率高于TS

2. fMP4片段更短，延迟更低

3. fMP4 的自适应码率比TS格式更流畅。片段的元数据集中在一起，解码更快，找关键帧位置更快。

4. TS容错高于fMP4

5. TS兼容性好

6. TS实现更简单

## RTSP实时流协议

RTSP是一个用于控制多媒体流传输的应用层协议，使用TCP进行连接，RTSP 提供了对流媒体服务器的远程控制功能，允许客户端进行播放、暂停、停止等操作。所有操作基于应答机制完成，是一个基于文本的协议，每行语句是\r\n(CRLF)结束。

每个请求基于会话进行：

1. 建立TCP连接
2. 发送options请求，查询服务端支持的请求。
3. 发送describe请求，获取媒体资源的描述信息。描述用SDP格式表示，由多组键值对组成。
4. 发送setup请求：根据describe响应中的SDP信息，发送 `SETUP` 请求来为特定的媒体流（如音频或视频）指定传输机制（如 RTP）和端口等参数。服务器为请求的媒体流创建会话，并返回一个唯一的会话ID。
5. 客户端发送 `PLAY` 请求，要求服务器开始传输媒体数据。服务器响应后，通过 RTP 协议开始传输媒体内容。客户端可以开始接收和播放媒体流。

**支持的方法**：options、describe、setup、play、pause、teardown(终止会话)、announce(通知服务器或客户端关于媒体描述的变化)、record(客户端请求开始向服务器发送媒体流).

RTSP不支持seek操作，但是可以使用play中的range字段，指定开始和结束播放的位置。

**适用场景**：实时监控、IP摄像头视频流传输、工业应用。延迟1-5S

**优点：**①实时性强：使用RTP传输，基于UDP，传得快。②控制灵活，支持多种控制流的方法。③多客户端支持。④扩展性好，可以和TLS结合进行加密。

### RTSP相对于Http流的区别？

**RTSP** 适合低延迟、实时交互控制要求高的应用场景，如视频监控、视频会议和实时直播等。它的控制灵活、延迟低，但实现和部署较为复杂，且在复杂网络环境中兼容性不如 HTTP。有防火墙问题。

**HTTP 流媒体**（如 HLS、DASH）更适合大规模分发、点播(拉流)，具有较好的网络兼容性和自适应码率支持。它通过高延迟换取了更高的稳定性和传输质量，尤其适用于互联网应用。

## RTP

![image-20240917152737469](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015018.png)

V:版本号; P:填充位; X:扩展位; CC:CSRC(特约信源)标识符个数; M: 标记，对视频表示1帧的结束，对音频表示开始; PT:有效载荷的类型(7位，98是h264)。 RTP报文的序列号。
时间戳、SSRC:同步信源、CSRC:特约信源，可以有0-15个。

1. 依赖UDP的一个传输层协议。RTP本身只保证实时数据的传输，并不能为按顺序传送数据包提供可靠的传送机制，也不提供流量控制或拥塞控制，它依靠RTCP提供这些服务。
2. RTCP（RTP Control Protocol）是RTP的一个补充协议，为RTP提供带外统计信息和控制信息。
3. RTP使用一个偶数端口号，而相应RTCP流使用下一个(奇数，递增)端口号。
4. RTP的优点是低延迟和高实时性，但缺点是不适合大规模数据传输，因为UDP不提供可靠性和有序传输。

SRTP是RTP的一个配置文件，为RTP数据提供加密、消息身份验证和完整性以及重放攻击保护等安全功能，SRTP有姊妹协议SRTCP。使用SRTP或SRTCP时，要启用消息身份验证功能。默认加密算法是AES，使用HMAC-SHA1算法计算数据内容摘要，用来验证完整性。SRTP只加密有效载荷部分。

## RTCP

RTCP会定期发送数据包计数、数据包丢失、数据包延迟变化、往返延迟时间等统计信息，向媒体参与者提供媒体分发中的质量保障。RTCP占用的带宽很小，总带宽的5%。RTCP不提供加密。RTCP的报告间隔随机，最小间隔为5S，通常频率不应该低于5秒一次。


## RTMP

RTMP（实时消息传输协议）是一种基于TCP的应用层协议，通过在客户端和服务器之间建立持久连接，提供稳定、低延迟的音视频实时传输服务。先建立TCP连接，然后三次握手建立流，然后开始通信。

RTMP的握手：①发送 `C0`，表示 RTMP 版本号。发送 `C1`（1536 字节），携带客户端时间戳、随机数及填充数据②发送 `S0`，表示 RTMP 版本号。发送 `S1`（1536 字节），携带服务端时间戳、随机数及填充数据③收到S0、S1发送C2，是S1的副本，服务端收到C0、C1发送S2，是C1的副本。

RTMP是一种流式传输协议，可以用多个流(视频、音频)分块传输**消息数据**，降低延迟。RTMP协议将音视频数据、命令消息等封装成消息使用流进行传输。消息头中包含了消息类型、大小、时间戳。分块儿 = 块头+数据。头中有流id、消息id。

优点:①低延时(优化后2-3S)②跨平台③可扩展④广泛的支持(主流CDN都支持RTMP)。缺点：①使用非公共端口1935，可能被防火墙拦截。②Flash逐渐淘汰，浏览器不再支持RTMP。③基于TCP，存在累计延时。RTMP主要用于大规模直播推流。

将RTMP的底层协议换成UDP可以将延迟降低到毫秒级，比如微信小程序。

**支持的方法：**connect、createStream、play、publish、pause、seek、deleteStream、closeStream、getStreamLength。

## SRT安全可靠传输

SRT基于UDP实现，支持在各种网络环境下进行低延迟、高质量的音视频传输，可以提供256为的AES加密。SRT在源和目标之间建立专用的通信链路，目标可以是服务器、CDN或者其它SRT设备。SRT有自己的拥塞控制算法。

优点：低延迟、高质量；轻松穿过防火墙；控制延迟以适应变化的网络条件；256位安全加密。

## HTTP-FLV

**HTTP-FLV** 是一种用于流媒体传输的协议，它结合了 HTTP 和 FLV格式，通过 HTTP 实时传输音视频数据。使用http+get从服务器拉流，获得flv格式的数据，太大的TAG会被切分，客户端组成完整的flv tag进行播放。2-5秒延时

HTTP-FLV可以在低延迟下进行实时流媒体传输，适合直播场景的客户端拉流。因为基于HTTP，所以兼容性很好，可以穿过防火墙，且易于部署。

## HLS

HLS是基于HTTP的流媒体传输协议，支持自适应码率，能够将音视频内容切片并逐段传输。特点：基于HTTP、分段传输、自适应码率、加密(AES-128)。

1. 基于HTTP协议：①通过现有的HTTP设施轻松部署②可以穿防火墙。

2. **分段传输**

   HLS协议的文件由两部分组成，TS和M3U8文件，M3U8是存储TS文件地址的索引文件。客户端获得索引文件就可以下载对应的TS文件进行播放，一般用于拉流点播。


HLS的自适应分辨率：一个视频片段有不同质量(分辨率)的TS文件，M3U8分两级，一级(主)M3U8存储二级(媒体)M3U8的URL。

![image-20240904155023756](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015142.png)

**动态选择质量**：客户端先下载主 M3U8 文件，然后根据当前的**网络带宽**和**设备条件**，选择合适的媒体 M3U8 文件（比如从 720p 开始）。
**实时调整**：如果网络状况恶化，带宽降低，客户端会切换到较低分辨率的 M3U8 文件（比如从 720p 切换到 480p）。反之，网络状况变好时，也可以从低分辨率切换到高分辨率。
**无缝切换**：客户端在播放一个 TS 文件时，已经下载了下一个 TS 文件。当检测到网络带宽变化时，它可以在播放完成当前 TS 文件后，从新的媒体 M3U8 文件中请求下一个 TS 文件，从而在不同质量级别间无缝切换。

## DASH 动态自适应流媒体传输

1. MPEG制定，用fMP4做封装格式。
2. 用MPD文件作为索引文件，可以实现自适应码率
3. 分片长度在2-10S
4. 支持多种加密方案，比如CENC来加密媒体内容。

选择 DASH 还是 HLS 取决于具体的应用需求、目标用户群体以及技术架构。如果兼容性是首要考虑，HLS 可能更合适；而如果灵活性和低延迟更重要，DASH 是不错的选择。

## 拉流、推流协议对比

播放拉流：HTTP-FLV(2-5秒延时)、HLS(5-30秒)、DASH(5-30S)

直播推流：

1. RTSP

   **优点**：基于RTP，延迟小(1S内)。**缺点**：①Web端兼容低②不容易与现代CDN集成③没有内置加密。 **应用场景**：安防、监控摄像头。适合内网推送视频。

2. RTMP

   **优点**：①实现简单，支持多(OBS、FFmpeg)②TCP可靠传输。③易与现代的CDN集成。**缺点**：①Flash淘汰，Web端支持变少②没有内置加密③基于TCP，延迟2-5S。**应用场景**：游戏直播、适合分发到大型流媒体平台。

3. WebRTC

   **优点**：①延迟低(100-500ms)②Web端支持③内置NAT穿透和加密机制。**缺点**：①交互复杂：信令交换、NAT穿透。②不适合大规模直播，适合点对点通信。③对网络环境敏感。**应用场景**：小规模实时交互、视频会议。

4. SRT

   **优点**：①基于UDP，抗丢包②有NAT穿透和加密③低延迟，通常在300ms到1秒 ④可以调整带宽利用。**缺点**：①在浏览器端原生支持较弱②计算资源要求较高③配置和部署较为复杂④生态还在发展。**应用场景**：低延迟、稳定性、高画质传输。

## 1.YUV与RGB的区别？

| <span style="display:inline-block;width:80px">类型</span> |                           **YUV**                            | **RGB**                                                      |
| ------------ | :----------------------------------------------------------: | ------------------------------------------------------------ |
| **定义**     | 一种颜色编码方式，将颜色分为**亮度（Y）和色度（U和V）分量**。 | 一种颜色编码方式，直接**使用红（R）、绿（G）、蓝（B）三原色**表示颜色。 |
| **用途**     |       广泛用于视频压缩和传输，如电视广播、视频编解码。       | 广泛用于显示设备和图像处理，如显示器、相机。                 |
| **颜色模型** |                 亮度（Luma）+ 色度（Chroma）                 | 红色（Red）、绿色（Green）、蓝色（Blue）                     |
| **优势**     | -  更适合人类视觉感知，**压缩效率高**。 - **可以独立处理亮度和色度**，有效减少带宽需求。 | -  直接对应显示设备的颜色显示，简单直观。 -  颜色信息不丢失，**适合图像处理和显示**。 |
| **表示方式** |              Y分量表示亮度，U和V分量表示色度。               | R、G、B三原色直接表示颜色。                                  |
| **色彩空间** |            非常适合**视频压缩和传输**，节省带宽。            | 非常适合**图像捕捉和显示**，颜色信息完整。                   |
| **应用场景** | -  视频编解码（如H.264、H.265） - 电视广播、视频会议等视频传输。 | -  显示器、相机等图像捕捉和显示设备。 - 图像编辑和处理软件。 |
| **压缩效率** |           高，尤其在对色度进行更强压缩时效果显著。           | 相对较低，不适合高效视频传输。                               |

RGB和YUV的转换：在显示之前进行格式转换

![image-20240927112837039](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015819.png)

YUV420P的存储格式

![image-20240926215342352](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015129.png)

YUV420SP的存储格式

![image-20240926215448683](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015893.png)

其他颜色空间：

1. HSV（Hue, Saturation, Value）

   HSV 是一种基于人类视觉感知的颜色模型，它更接近人类对颜色的感知方式。

   **Hue** (色相)：颜色的类型，如红色、蓝色等。

   **Saturation** (饱和度)：颜色的浓度或纯度，表示颜色的灰度成分。

   **Value** (亮度)：颜色的明暗程度，代表颜色的亮度。

   **用途**：HSV 颜色空间通常用于图像处理中的颜色选择、色彩分割等任务，尤其是在图像编辑工具中（如 Photoshop）经常使用。

2. CMYK（Cyan, Magenta, Yellow, Key/Black）

   CMYK 是一种减色模型，常用于打印和印刷。它与 RGB 是互补的颜色模型，表示通过减去光的某些部分来生成颜色。

   - **Cyan** (青色)
   - **Magenta** (品红)
   - **Yellow** (黄色)
   - **Key/Black** (黑色)

   **用途**：CMYK 主要用于纸上印刷过程，因为它与物理油墨的特性相符。每种颜色通过吸收光的不同波长来显示。

3. Lab (CIELAB)

   Lab 颜色空间是根据人类视觉特性设计的，是 CIE（国际照明委员会）推出的颜色空间之一。它试图模拟人眼对颜色的感知，并分为三个分量：

   - **L**：亮度分量（Lightness），表示颜色的明暗程度。
   - **a**：红绿分量，正值表示偏红，负值表示偏绿。
   - **b**：蓝黄分量，正值表示偏黄，负值表示偏蓝。

   **用途**：Lab 颜色空间广泛用于颜色校正和颜色匹配，尤其在印刷、图像扫描和相片修正中使用。

## 2.AAC与PCM的区别

1. **定义**  AAC是有损音频压缩编码格式; PCM是无损的音频数字化编码方式     
2. **音质**  AAC减少数据量的同时尽量保留音质  PCM有完整的音质 
3. **文件大小**  AAC较小，适合存储和传输  PCM较大，不适合传输和存储  
4. **使用场景**  AAC:流媒体、在线音乐、手机铃声等  PCM:音频处理、音频采样、音频分析等  
5. **采样率**  AAC可变，根据需要进行压缩  PCM固定，通常为44.1kHz、48kHz、96kHz等  
6. **比特率**  AAC可变比特率（VBR）或恒定比特率（CBR）  PCM固定，根据量化位数和采样率确定 
7. **编码复杂度**  AAC较高，需要进行复杂的压缩算法  PCM较低，直接采样量化  
8. **解码复杂度**  AAC较高，需要进行复杂的解码算法  PCM较低，直接反量化和解码  **延迟**  较低，适合实时传输  较高，不适合实时传输

## 3.Opus音频编码与AAC

2012年提出Opus，Opus 结合了两个现有的编解码器：CELT（Constrained Energy Lapped Transform）和 SILK（用于语音编码）。Opus 的设计目标是针对语音和音乐提供灵活、低延迟的压缩，适应性更强。

**AAC** 主要应用于音乐、视频流媒体和文件存储，注重音频质量。

1. **标准 AAC**：100-200 毫秒的延迟，主要用于非实时的音频传输和存储。
2. **AAC-LD（低延迟AAC）**：约 20-50 毫秒的延迟，适用于实时语音通信。
3. **AAC-ELD**(增强版低延迟AAC)：约 15-30 毫秒的延迟，适用于要求更高音质的实时通信。

**Opus** 主要用于实时语音通信和音频流，注重低延迟和灵活的比特率控制。

①Opus 的最低延迟可以达到 **5 ms**；②Opus 是开源项目，没有专利限制；③Opus 支持动态调整比特率，在网络条件变化的情况下能够自适应调节音频质量，确保在不同带宽下提供最佳的听觉体验。



## 4.RTP、RTSP、RTMP协议对比？

| <span style="display:inline-block;width:80px">特性</span> | **RTP**                                        | **RTSP**                                   | **RTMP**                                                    |
| --------------------------------------------------------- | ---------------------------------------------- | ------------------------------------------ | ----------------------------------------------------------- |
| **定义**                                                  | 实时传输协议，用于在网络上传输实时数据         | 实时流传输协议，用于**控制流媒体服务器**   | 实时消息传输协议，**用于流媒体传输**                        |
| **用途**                                                  | 主要用于传输音频和视频数据                     | 主要用于建立和控制多媒体流会话             | 主要用于传输音频、视频和数据                                |
| **传输层**                                                | 基于UDP                                        | 基于TCP/UDP                                | 基于TCP                                                     |
| **实时性**                                                | 高                                             | 中等                                       | 高                                                          |
| **传输方式**                                              | 数据包传输，支持多播                           | 控制消息传输，管理流会话                   | 数据包传输，支持持久连接                                    |
| **适用场景**                                              | **视频会议**、VoIP、实时流媒体传输             | 点播和直播流媒体控制、流媒体会话管理       | 直播流、互动直播、实时通信                                  |
| **特点**                                                  | 支持实时数据传输，低延迟，支持QoS              | 提供流会话的建立、控制和终止，类似HTTP协议 | 支持持久连接和低延迟传输，适用于**互动应用**                |
| **扩展性**                                                | 通过RTCP提供控制功能，支持多种编码格式         | 支持通过扩展头和参数实现灵活控制           | 支持自定义消息类型和扩展                                    |
| **协议关系**                                              | 与RTCP配合使用，提供质量反馈和控制信息         | 与RTP配合使用，控制RTP传输的音视频流       | 常与Flash Player和Adobe Media Server配合使用                |
| **优缺点**                                                | 优点：低延迟、高实时性；缺点：不适合大规模传输 | 优点：灵活控制、多功能；缺点：较高的复杂性 | 优点：低延迟(1-3s)、实时性强；缺点：依赖TCP连接，扩展性有限 |

1. RTMP（实时消息传送协议包头+包体）
   1. 特点：基于TCP传输，可以持久连接和低延迟传输，应用于flash player和adobe media server
   2. 优点：
      1. 专为流媒体开发的协议
      2. 与FLV的生态相得益彰
      3. 延迟低、高实时
   3. 缺点
      1. 基于TCP，链路抖动的时候不能很好的使用带宽
      2. 使用非公共端口，可能被防火墙拦截
      3. adobe的私有协议，浏览器生态不友好。
2. RTP
   1. 依赖UDP的一个传输层协议。RTP本身只保证实时数据的传输，并不能为按顺序传送数据包提供可靠的传送机制，也不提供流量控制或拥塞控制，它依靠RTCP提供这些服务。
   2. RTCP（RTP Control Protocol）是RTP的一个补充协议，负责对RTP的通讯和会话进行带外管理（如流量控制、拥塞控制、会话源管理等）。
   3. RTP使用一个偶数端口号，而相应RTCP流使用下一个(奇数，递增)端口号。
   4. RTP的优点是低延迟和高实时性，但缺点是不适合大规模数据传输，因为UDP不提供可靠性和有序传输。
3. RTSP
   1. RTSP通常与RTP配合使用，RTSP负责控制流媒体会话，而RTP负责实际的音视频数据传输。

## 5.直播使用什么协议？为什么？

直播使用RTMP协议。优点如下

1. **低延迟。**RTMP协议在传输数据时采用了分段发送的方式，这使得音视频数据能够以较低的延迟传输，满足实时互动的需求。RTMP通过保持一个长连接，并且利用小的分段块（通常是128字节）传输数据，确保了数据的及时传递和较低的传输延迟。
2. **高效的传输。**RTMP使用TCP连接来传输数据，通过TCP连接，RTMP能够进行可靠的数据传输，确保音视频流的完整性和连续性，减少了数据包丢失的可能性。
3. **广泛的支持。**RTMP协议得到了广泛的支持，被许多音视频直播平台和流媒体服务器所采用，如YouTube、Twitch、Facebook Live等。RTMP协议的实现已经集成在很多音视频处理软件和硬件中，使用起来方便，开发者可以直接利用现有的RTMP库和工具进行开发和部署。
4. **简单的推流机制。**RTMP协议的推流过程相对简单，常用的推流软件如OBS、FFmpeg等都支持RTMP协议，主播只需配置推流URL和密钥即可开始推流。
5. **扩展性好。**通过RTMP的元数据功能，可以在音视频流中嵌入诸如字幕、互动信息、统计数据等，增强了直播的互动性和可定制性。
6. **兼容性好。**RTMP协议具有良好的兼容性，可以与多种音视频编码格式和封装格式结合使用，如H.264视频编码和AAC音频编码。

## 6.RTMP消息的优先级？

RTMP是包头+包体，包头有四种类型，12, 8, 4, 1 个字节。第一个字节的前2个bit决定了包头类型，后6个bit决定了channelid(标识数据流)。包头还有时间戳、数据大小、数据类型、流ID。数据超过128Byte的时候就可能有多个rtmp包组成

**RTMP**将消息(数据)分为几种类型：

1. **音频消息**：通常具有较高的优先级，因为音频流的连续性对用户体验影响较大。
2. **视频消息：**关键帧比非关键帧具有更高的优先级，因为关键帧对视频解码至关重要。
3. **命令消息**：如播放、暂停等控制命令，通常具有最高优先级，以确保用户操作的响应性和流的控制逻辑。
4. **数据消息**：如元数据和其他非媒体内容，优先级可能较低，但这取决于具体内容和应用场景。

在RTMP协议中，并**没有明确指定每种消息的固定优先级**，而是**需要根据实际应用的需求来动态调整**。实施优先级管理的一般方法包括：

1. **优先级标记：**在实际开发中，可以在**消息头部标记优先级**，这个标记可以帮助决定消息的发送顺序和处理优先级。
2. **带宽管理：**服务器根据当前的带宽条件和各种消息的优先级，动态调整数据的发送策略。在带宽受限时，确保高优先级消息的传输。
3. **客户端处理：**客户端在接收到消息时，也应根据优先级进行适当的处理，优先解码和显示高优先级的内容，以保证流畅的播放体验。
4. **网络状况适应：**在网络状况变化时，动态调整消息优先级的策略，如在网络状况不好时提高关键帧的优先级，保证视频播放的连续性。

示例1：体育直播

**假设有一个重大的体育赛事直播，其中实时性和视频质量是用户体验的关键。在这种情况下，RTMP服务器和客户端可能采用以下策略：**

**1**）高优先级处理关键帧和音频数据：为了保证视频流的连续性和减少延迟，**关键帧（I帧）和音频数据**会被赋予最高的传输优先级。这样做可以在网络状况波动时确保视频的基本观看质量和音频的连续播放。

**2**）动态调整非关键帧优先级：非关键帧（P帧和B帧）的优先级**根据当前网络带宽动态调整**。在带宽充足时，这些帧被正常发送以提高视频质量；在带宽紧张时，部分非关键帧可能会被延迟发送或丢弃，以确保关键帧和音频的流畅传输。

示例2：在线讲座

**在一个在线教育平台的直播讲座中，除了视频和音频数据外，可能还会有实时的文字和图表数据（如PPT展示）需要传输。在这种场景下，优先级的分配可能如下：**

**1**）最高优先级给予**音频和控制消息**：音频的连续性对于理解讲座内容至关重要，因此音频数据会被赋予最高优先级。同样，播放控制消息（如播放、暂停命令）也需要快速响应。

**2**）高优先级给予**关键帧和关键数据消息**：视频的关键帧和同步的PPT页更改命令也非常重要，因此这些数据类型会有较高的优先级。

**3**）适中优先级给予非关键视频帧：非关键的视频帧（如P帧和B帧）则可以在保证了关键内容传输的基础上，根据带宽情况进行调整。

## 7.RTMP 消息分优先级的设计有什么好处？

1. **提高关键数据的传输效率**

   在RTMP流中，不同类型的数据（如关键帧、音频数据、视频数据和控制信息）对流媒体的连续性和质量有不同的影响。通过为这些消息类型设置不同的优先级，可以确保在网络状况不佳时，**最重要的数据（如关键帧和音频数据）优先传输**，从而维持视频播放的连续性和减少卡顿。

2. **优化带宽使用**

   通过对消息进行优先级排序，RTMP可以**更智能地分配带宽资源**。在带宽受限的情况下，**高优先级的消息（如关键帧和音频）可以被优先发送**，而低优先级的消息（如非关键帧）可能会被延迟处理或丢弃，这样做可以最大化关键内容的传输效果。

3. 适应网络波动

   在网络状态不稳定或波动较大的环境下，优先级设计允许**RTMP动态调整数据传输策略**。例如，在网络状况突然变差时，系统可以临时提高关键数据的优先级，以保证核心体验不受影响。

4. 改善用户体验

   用户体验在流媒体服务中至关重要。通过确保音视频流的关键部分优先处理，可以显著减少缓冲时间和提高播放质量。这对于维持观众的参与度和满意度非常重要，特别是在直播事件中。

5. 提高系统的可伸缩性

   对消息进行优先级分类还可以提高系统的可伸缩性。在面对大量并发连接和数据流时，优先级系统可以帮助服务器更有效地管理资源，确保所有用户都能获得尽可能好的服务。例如，当服务器负载接近极限时，可以优先处理和发送高优先级的流量，**确保关键服务不会中断**。

## 8. 为什么点播使用RTSP协议

1. 实时性与交互性播放控制：

   交互性：RTSP协议类似于HTTP协议，它更适合流媒体数据的传输和控制。RTSP提供了控制功能，如播放、暂停、停止、快进、快退等。这些功能对于点播系统至关重要，因为用户需要灵活地控制视频的播放。

   实时性：RTSP能够与RTP结合使用，以提供高效的流媒体传输。RTP负责实际的数据传输，而RTSP负责控制流媒体会话，从而实现实时播放和控制。

2. **分离控制信道与数据信道**

   控制信道与数据信道分离：RTSP将控制信道和数据信道分离，这意味着控制指令（如播放、暂停）和实际的音视频数据传输是通过不同的通道进行的。这种分离**提高了传输效率，并减少了控制指令对数据传输的干扰。**

3. 支持多种传输方式

   **传输灵活性：**RTSP可以与RTP和RTCP一起使用，通过UDP或TCP进行传输。RTP用于实际的数据传输，而RTCP用于监控数据传输的质量和性能。这种灵活性使得RTSP能够适应不同的网络环境和应用需求。

   RTCP的作用：RTCP与RTP一起使用，主要用于提供反馈信息，如丢包率、延迟、抖动等，帮助优化数据传输和改善用户体验。

4. 网络适应性

   自适应码率：RTSP支持自适应码率流媒体传输（ABR），播放器能够根据网络条件动态调整音视频流的码率，以提供流畅的观看体验。

   缓冲与预加载：RTSP协议允许在播放前进行缓冲和预加载，以确保在播放过程中不会出现卡顿和延迟。缓冲机制能够有效应对网络波动和延迟问题。

5. **点播应用的典型场景**

   视频点播平台：RTSP协议在视频点播平台上广泛使用，如腾讯视频、bilibili等。这些平台需要支持大量用户同时访问，并且每个用户都需要灵活地控制视频播放。

   教育和培训：在教育和培训视频点播中，用户需要灵活地控制视频播放，以便进行学习和复习。

6. RTP与RTCP的角色

   **RTP**（Real-time Transport Protocol）：负责音视频数据的传输，提供时间戳和序列号，用于同步和重组数据包。

   **RTCP**（Real-time Control Protocol）：与RTP一起使用，主要用于监控数据传输的质量和性能。RTCP定期发送控制包，提供统计和控制信息，如丢包率、延迟、抖动等，帮助发送方和接收方优化数据传输。

## 9. RTP与RTCP的作用及其与RTSP的关系

**(1)RTP**

1. **数据传输**：RTP主要用于传输音视频数据，提供了时间戳和序列号，用于实现数据包的有序传输和同步。
2. **低延迟**：RTP在传输时采用UDP协议，可以减少传输延迟，适用于实时音视频传输。
3. **实时性**：通过RTP传输的数据可以实现实时播放，适用于点播和直播的需求。

**(2)RTCP**

1. 质量控制：RTCP用于传输控制信息，监控RTP数据传输的质量，如数据包丢失率、延迟、抖动等。
2. 同步管理：RTCP帮助维护音视频同步，通过报告机制提供传输质量反馈，以便进行相应的调整和优化。
3. 会话管理：RTCP支持会话成员的报告，提供关于会话参与者和媒体流的统计信息。

**(3)RTSP与RTP/RTCP的关系**

**1**）RTSP控制：RTSP负责控制流媒体会话的创建、维护和终止，通过发送控制命令（如PLAY、PAUSE、TEARDOWN等）来管理音视频流。

**2**）RTP传输：在RTSP的控制下，实际的数据传输由RTP协议完成，RTP提供了高效的实时音视频传输机制。

**3**）RTCP反馈：RTCP与RTP协同工作，为RTSP会话提供传输质量的实时反馈，帮助优化传输和播放效果。

| **特性**     | **RTSP**                | **RTP**            | **RTCP**           |
| ------------ | ----------------------- | ------------------ | ------------------ |
| **主要功能** | 控制流媒体会话          | 实时传输音视频数据 | 监控和报告传输质量 |
| **传输协议** | TCP/UDP                 | UDP                | UDP                |
| **应用场景** | 点播系统                | 实时音视频传输     | 质量监控与同步管理 |
| **控制命令** | PLAY、PAUSE、TEARDOWN等 | 无                 | 质量报告、统计信息 |
| **优点**     | 灵活控制、交互性强      | 低延迟、高效传输   | 提供反馈、优化传输 |

# 直播与点播

## 1. 直播与点播的区别？

**(1)传输模式**

1. **直播：**通过采集音视频数据，进行实时编码和压缩，**通过网络实时传输给观众**，观众几乎同时接收到音视频内容。

 	2. **点播：**音视频内容已经被录制、编码并存储在服务器上，观众可以随时请求播放，**服务器将数据传输给观众**，观众可以在任何时间点开始观看。

**(2)延迟**

1. **直播：**通常具有较低的延迟。应用：适用于实时互动和即时反馈的场景，如体育赛事、新闻直播、在线教育、直播购物等。
2. **点播：**延迟相对较高，因为**数据是预先录制和存储的**，观众可以随时开始观看。应用：适用于非实时性要求的场景，如电影、电视剧、纪录片等。

**(3)技术实现**

1. **直播：**编码和传输：实时采集、编码、传输，使用自适应比特率（ABR）技术，常用的协议包括RTMP、HLS、DASH等。延迟控制：通过优化编码、传输和播放链路，尽量减少延迟。技术挑战：网络抖动、带宽波动、数据丢包等对直播质量的影响较大，需要采用抗抖动、纠错和自适应技术。
2. **点播：**编码和传输：预先录制、编码并存储，观众按需请求数据，使用CDN加速分发，常用的协议包括HTTP、HLS、DASH等。缓存机制：通过合理设置缓冲区大小，保证播放的流畅性。技术挑战：内容存储和分发的效率、用户请求的并发处理能力、不同终端和网络环境下的适配性。

**(4)应用场景**

1. **直播：**典型应用：体育赛事直播、新闻直播、在线教育直播、直播购物、娱乐直播（如游戏直播、演唱会直播）等。
2. **点播：**典型应用：在线视频平台（如抖音、B站、腾讯视频）、教育视频点播、企业培训视频点播等。

## 2. 直播技术的流程？

![image-20240917155247974](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015853.png)

**(1)音视频采集**

​	摄像头捕捉视频画面，麦克风捕捉音频。采集卡或计算机将模拟信号转换为数字信号，并进行初步处理。

**(2)音视频处理**

1. 预处理：包括降噪、增益调整、白平衡调整等，以提高音视频信号的质量。
2. 同步处理：确保音视频同步，避免出现画面和声音不同步的情况。

**(3)编码**

1. 视频编码：将原始视频信号压缩为H.264、H.265等格式，以减少数据量。
2. 音频编码：将原始音频信号压缩为AAC、MP3等格式，以减少数据量。多码率编码：生成多种不同分辨率和码率的流，以适应不同网络环境。

**(4)推流**

​	将编码后的音视频流通过推流协议发送到流媒体服务器。流媒体服务器接收并存储音视频流，准备进行分发。

**(5)流媒体服务器**

1. 接收推流：接收来自客户端的推流数据。
2. 存储和缓存：存储音视频数据，并进行缓存，以平衡负载和提高分发效率。
3. 转码和转封装：根据需求进行转码（如从H.264转为H.265）和转封装（如从RTMP转为HLS）。
4. 分发：将音视频流分发到内容分发网络（CDN）。

**(6)内容分发网络（CDN）**

1. 边缘节点缓存：将音视频流缓存到靠近用户的边缘节点，减少延迟和缓解服务器压力。
2. 负载均衡：根据用户的地理位置和网络状况，智能选择最佳的边缘节点进行分发。
3. 传输优化：通过协议优化和带宽管理，提高传输效率和可靠性。

**(7)解码与播放**

1. 拉流：播放器从CDN或流媒体服务器拉取音视频流。
2. 解码：将压缩的音视频流解码为可播放的音视频信号。
3. 同步播放：确保音视频同步播放，提供流畅的观看体验。

## 3. 直播怎么做到首屏秒开？

**(1)低延迟编码与传输**

1. **低延迟编码**：使用硬件编码器（如GPU、ASIC）进行实时编码，减少编码时间。调整编码参数（如降低GOP大小、减少B帧等）以减少延迟。
2. **低延迟传输协议：**
   1. RTMP：虽然传统，但经过优化配置后仍能提供较低延迟。
   2. SRT（Secure Reliable Transport）：提供更高可靠性和更低延迟的传输。
   3. WebRTC：一种用于实时通信的协议，具有极低延迟，非常适合直播。

**(2)缓冲区优化**

1. **预缓冲：**最小化初始缓冲：减少播放器的初始缓冲区大小，使视频尽快开始播放。智能缓冲策略：根据网络状况动态调整缓冲区大小，确保平衡流畅播放和低延迟。
2. **快速缓冲：**优先传输首屏所需的数据包，确保视频尽快解码和播放。

**(3)自适应比特率（ABR）技术**

1. **初始码率选择：**
   1. 低码率启动：在直播开始时，选择较低的码率和分辨率，以保证视频流能快速加载和播放。
   2. 带宽检测：通过检测用户当前的网络带宽，选择合适的初始码率。
2. **动态码率调整：**根据网络状况和带宽变化，动态调整视频流的码率和分辨率，保证流畅播放。

**(4)内容分发网络（CDN）**

1. **边缘节点部署：**
   1. 靠近用户：将直播流缓存到靠近用户的边缘节点，减少传输延迟。
   2. 负载均衡：通过智能调度，将用户请求分配到最优的边缘节点，确保快速响应。
2. **CDN预热：**在直播开始前，将视频流预热到CDN边缘节点，减少用户首次请求的延迟。

**(5)预加载与预拉流**

1. **预加载技术**：

   页面加载前预拉流：在用户进入直播页面前，提前拉取直播流数据并进行缓冲。

2. **后台加载**：

   在用户点击播放前，**后台静默加载视频流**，待用户点击时立即播放。

**(6)优化播放器**

1. 播放器优化：使用优化后的轻量级播放器，减少初始化时间。
2. 快速解码：优化解码器，提高视频数据的解码速度，确保视频流能迅速开始播放。

**(7)辅助技术**

1. 多路复用：使用HTTP/2或QUIC协议，提高数据传输效率和速度。
2. 低延迟传输：QUIC协议具有更低的传输延迟，适用于直播视频的快速传输。
3. 流分片技术：将视频流分割为更小的片段，减少每次加载的数据量，加快播放速度。

## 4. 为什么一些平台直播主播和用户几乎同步，一些平台主播和用户有延时差别？

1. 流媒体协议不同

   使用不同的协议会直接影响延时，低延迟协议优先用于实时互动，而高延迟协议用于稳定的大规模直播场景。

2. CDN（内容分发网络）的使用

   CDN的缓存机制和分发策略决定了延时长短，优化CDN可以减少延时，但未优化的情况下延时可能会更大。

3. 平台的延迟优化策略

   平台通常会有缓冲机制来应对网络波动。为了减少卡顿，一些平台会设置较长的缓冲时间，这会增加延迟。而其他平台会缩短缓冲时间以提高实时性。

## 5.海量用户看直播，负载均衡怎么设计？

1.  CDN（内容分发网络）引入

   CDN通过将内容缓存至全球范围内的边缘节点，将用户请求分配到离用户最近的节点，以减少带宽压力、提高访问速度并降低延迟。

2. 水平扩展和集群化架构

   使用多台服务器组成**流媒体集群**来处理不同区域或类型的用户流量。通过水平扩展（Scale-out），将流媒体服务水平扩展到多个服务器节点。分成推流集群和分发集群。

   DNS负载均衡：通过DNS负载均衡用户访问不同的直播服务器，提升容灾能力。当某个服务器出现问题时，DNS会将用户流量引导至其他正常工作的服务器。

   反向代理负载均衡：使用反向代理服务器(Nginx）根据流量压力，将流量均匀分配到不同的流媒体服务器。

3. 直播流的分片和多路推流

   将直播流进行分片并将其推送到多个服务器或CDN节点，能够有效分散流量。

   根据不同用户的网络带宽，提供多种码率（如高清、标清、低清）的直播流。负载均衡可以根据用户带宽或需求将不同的码率分配到不同的服务器上。

4. 缓存机制

   在流媒体服务器内部引入缓存，将热门片段保留在内存或快速存储中，减少磁盘I/O和数据库查询。

## 6.音视频直播系统的基本架构？

![Snipaste_2024-07-24_16-51-19](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015445.png)

# 播放器

## 0.处理流程

![image-20240926200546124](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015094.png)

## 1. 音频播放器的基本架构？

![Snipaste_2024-07-24_16-52-24](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012015254.png)

1.用户界面（UI）

​	提供播放控件（如播放、暂停、停止按钮）和进度条。显示当前播放时间、总时长和音量控制。允许用户选择文件或输入流媒体URL进行播放。

2.控制器（Controller）

​	管理播放状态（如播放、暂停、停止）和用户输入事件。协调各模块之间的通信，确保音视频流的顺利播放。

3.网络模块（Network）

​	下载和缓冲流媒体数据，处理网络不稳定和带宽波动。支持断点续传和动态自适应流切换（如HLS和DASH）。

4.解封装（Demuxer）

​	解析多媒体容器格式，提取音视频流并交给解码器处理。支持本地文件和网络流的解析，处理多种封装格式。

5.解码器（Decoder）

​	将压缩的音视频数据解码为原始数据（PCM音频、YUV视频帧）。处理多种编码格式，提供高效的解码算法。

6.缓冲区（Buffer）

​	存储解码后的音视频数据，平滑播放并处理网络延迟。管理数据的读取和写入，确保播放过程中的连续性。

7.同步模块（Sync）

​	**使用时间戳（PTS、DTS）或系统时钟来保持音视频同步**。处理播放过程中音视频不同步的问题，调整播放速度或时间偏移。

8.音频渲染器（Audio Renderer）

​	将解码后的PCM音频数据发送到音频设备进行播放。控制音量、音频效果（如均衡器、混响）等。

9.视频渲染器（Video Renderer）

​	将解码后的YUV视频帧转换为RGB并显示在屏幕上。支持视频缩放、旋转、滤镜效果等。

## 2.基于libVLC的播放器

库中最重要的是libvlc和libvlccore

基本使用：

```cpp
1.创建一个libVLC 实例。初始化 VLC 实例，可以配置启动设置。
libvlc_instance_t *vlcInstance = libvlc_new(0, nullptr);
2.创建媒体播放器：创建一个对象作为媒体播放器。
libvlc_media_player_t *mediaPlayer = libvlc_media_player_new(vlcInstance);
3.加载视频文件，可以是文件、http地址、rtsp地址、录屏
libvlc_media_t *media = libvlc_media_new_path(vlcInstance, "video.mp4");
将媒体对象设置到播放器
libvlc_media_player_set_media(mediaPlayer, media);
```

播放器：

1. 视频显示

   ```cpp
   1.设置媒体
       m_media = libvlc_media_new_path(vlcInstance, "video.mp4");
   2.根据媒体设置播放器
       m_player = libvlc_media_player_new_from_media(m_media)
       3.根据窗口的纵横比，设置播放器的纵横比
       libvlc_video_set_aspect_ratio(m_player, strRatio.c_str());
   4.将媒体播放器的渲染目标设置为指定的窗口
       libvlc_media_player_set_hwnd(m_player, m_hwnd);
   ```
2. 播放控件

   ```cpp
   1.play  播放
       libvlc_media_player_play(m_player);
   2.pause 暂停
       libvlc_media_player_pause(m_player);    
   3.stop  停止
       libvlc_media_player_stop(m_player);
   4.设置进度
       libvlc_media_player_set_position(m_player, pos);
   5.设置音量
       libvlc_audio_set_volume(m_player, volume);
   6.得到视频时长
       libvlc_media_player_get_length(m_player);
   ```
   

## 3.FFmpeg实现播放器

​	音视频同步、暂停播放/恢复播放、全屏、获取时长、播放事件回调。

播放器开发通常需要从流媒体或文件中解码视频和音频数据，然后将解码后的数据渲染到屏幕上或者输出到扬声器。SDL 在这些过程中提供了便捷的接口来管理这些任务。

使用 SDL 创建一个显示窗口，并通过 SDL 的 **texture**（纹理）和 **renderer**（渲染器）来管理图像的显示。将解码后的图像帧上传到 SDL 的纹理中，然后通过渲染器将其渲染到窗口上。

## 4.SDL库

### 1.初始化SDL

```cpp
SDL_Init(SDL_INIT_EVERYTHING);//初始化所有模块
模块有：定时器、音频设备、图像渲染模块、时间循环模块、所有模块
```

### 2.渲染图像

![image-20240923205534543](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012016411.png)

```cpp
1. 创建一个 SDL 窗口
根据现有的窗口句柄创建：
m_sdlWindow = SDL_CreateWindowFrom(m_nativeHandle);
创建一个平台独立的渲染窗口：居中、默认宽高、大小可调
m_sdlWindow = SDL_CreateWindow("ffmpeg-simple-player",SDL_WINDOWPOS_CENTERED,SDL_WINDOWPOS_CENTERED,SDL_WINDOW_DEFAULT_WIDTH,SDL_WINDOW_DEFAULT_HEIGHT,SDL_WINDOW_RESIZABLE);
2. 根据渲染窗口，创建一个渲染器，-1表示SDL自己决定渲染方式
m_sdlRender = SDL_CreateRenderer(m_sdlWindow, -1, SDL_RENDERER_ACCELERATED);有软件渲染、GPU加速渲染、垂直同步渲染、离屏渲染，这里是GPU加速渲染
3. 设置渲染窗口逻辑大小：
SDL_RenderSetLogicalSize(m_sdlRender,SDL_WINDOW_DEFAULT_WIDTH, SDL_WINDOW_DEFAULT_HEIGHT); 
4. 设置渲染缩放质量，"1"表示用了线性插值
SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, "1");
销毁渲染器
5. 创建 SDL 纹理：纹理是流式的，动态更新纹理的场景(适用于视频播放)
SDL_Texture *tex = SDL_CreateTexture(m_sdlRender, SDL_PIXELFORMAT_RGB24, SDL_TEXTUREACCESS_STREAMING, w, h);
6.锁定 SDL_Texture，以便直接访问其像素数据。 
//获取指向纹理像素数据的指针给pixels，返回每行像素数据的字节数给pitch
SDL_LockTexture(tex, NULL, &pixels, &pitch);
// 将传来的pixeldata复制到pixels里边。
memcpy(pixels, pixelData, pitch * rows);
SDL_UnlockTexture(tex);
7.清空渲染器，确保新帧的内容不会受到之前帧的影响。
SDL_RenderClear(m_sdlRender);
8.将纹理复制到渲染器  null表示：绘制整个纹理，渲染整个窗口
SDL_RenderCopy(renderer, texture, NULL, NULL);
9.将渲染器的内容呈现到窗口
/*SDL库中用于将渲染器中的内容呈现到窗口上的函数*/
SDL_RenderPresent(m_sdlRender);
```

SDL提供多缓冲机制（通常是双缓冲）可以提高渲染性能，减少屏幕撕裂。渲染函数会将图像渲染到后缓冲区，只有在调用 `SDL_RenderPresent` 时，后缓冲区的内容才会显示到屏幕上。通过启用 `SDL_RENDERER_PRESENTVSYNC`(垂直同步渲染)，可以让渲染与屏幕的垂直刷新同步，进一步减少撕裂现象。

### 3.SDL音频播放

音频数据传输：主动拉数据、被动接数据。播放器使用拉数据的方式，通过声卡驱动主动回调的方式传输数据，这个特性适合把音频设备时钟作为音视频同步的主时钟。

![image-20240923213635110](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012016465.png)

```cpp
使用 SDL 初始化音频子系统。
    
设置音频回调函数，让 SDL 在需要数据时调用该回调函数。
将解码后的音频数据通过回调函数传递给 SDL，由 SDL 播放音频。
    
1. 打开音频设备并初始化音频子系统的函数；
// 打开默认设备；0是播放(输出音频)；wanted_spec指定音频设备的期望格式；实际提供的音频格式设置成null；是否允许改变指定的音频参数
SDL_OpenAudioDevice(NULL, 0, wanted_spec, NULL, 0);    
SDL_AudioSpec wanted_spec;
wanted_spec.freq = 44100;        // 音频采样率
wanted_spec.format = AUDIO_S16SYS; // 音频格式
wanted_spec.channels = 2;         // 声道数量
wanted_spec.silence = 0;
wanted_spec.samples = 1024;       // 缓冲区大小
wanted_spec.callback = audio_callback;  // 设置音频回调函数  
2.播放和暂停：0是播放，1是暂停
SDL_PauseAudio(0);
```

### 4.事件循环

SDL将事件放到了事件队列，查询队列得到发生的事件。事件类型：鼠标、键盘、窗口、自定义，用SDL_Event(union联合体)存放事件类型。

事件循环使用注册的方式，将事件类型和事件回调函数放到map表中。处理事件的方式有三种：

1. SDL_WaitEvent: 不带超时的持续事件等待，没事件就阻塞。
2. SDL_WaitEventTimeout:带超时的事件等待，没有事件，超时之后返回。
3. SDL_PollEvent:使用事件轮询机制，有事件返回1，无时间返回0。

SDL 提供了 `SDL_USEREVENT` 类型来表示用户定义的特殊事件，事件的相关数据可以通过 `SDL_Event` 结构中的 `user` 字段传递。

根据事件类型调用对应的函数。

### 5.定时器

使用定时器来定时刷新播放器的图像。

```cpp
1.生成定时器
    // 定时器间隔时间、到时间的回调函数，传递给回调函数的用户自定义参数
	SDL_TimerID timerId = SDL_AddTimer(interval, callbackfunc, cb);
2. 回调函数
    Uint32 callback(Uint32 interval, void* param);
3. 定时器删除：用于删除已创建的定时器，防止定时器再度触发。
    SDL_RemoveTimer() 
```

在播放器中，通过 SDL 的定时器机制定时触发事件。在 SDL 中，定时器的回调函数会在特定的时间间隔后被调用，而这个回调函数的作用是在定时器触发时向 SDL 的事件队列中推送一个自定义事件 (`SDL_USEREVENT`)。这里定时回调的参数是一个void返回值的函数。

#### 避免定时器回调函数在独立线程中执行的复杂性:
​	使用 `SDL_PushEvent()` 将定时器触发的事件发送到 SDL 的事件队列中，然后在事件循环中处理这些事件。通过这种方式，**定时器的回调函数实际上不会直接执行任何重要的逻辑，而是简单地将事件推送到事件队列中**。事件处理逻辑将在事件循环所在的线程（通常是主线程）中执行。

## 5. 播放器解码和展示

![image-20240924112355822](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012016817.png)

播放器使用多线程，包括：事件循环线程(主线程)、demux线程、视频解码线程、音频播放线程。

1. 在主线程中启动事件循环管理。

2. 初始化渲染窗口、渲染器。

3. 设置定时器，定时30ms，定时添加事件(显示渲染内容)。

4. 播放器：

   1. 设置文件地址

   2. 设置视频帧回调函数和回调数据(用于渲染器显示)

   3. 初始化播放器

      创建解封装线程、创建音频解码线程、创建视频解码线程、打开默认音频设备并初始化、注册视频帧刷新事件、注册键盘事件(`av_seek_frame()` 函数重新定位到指定的时间点,跳转之后刷新数据队列)。

   4. 启动播放器

      启动解封装线程、启动视频解码线程、启动音频解码线程、音频播放启动、40ms定时器调度刷新。

5. 事件循环检测处理。

### 视频帧刷新

1. 音视频同步：从视频帧队列中取出视频帧，计算两帧之间的延迟-delay,计算视频和音频的时间差-diff。根据调整后的延迟，确定后续视频帧更新到纹理的延迟。

   ```cpp
   /*计算同步阈值（sync_threshold），如果延迟值大于 AV_SYNC_THRESHOLD，则使用延迟值作为同步阈值，否则使用 AV_SYNC_THRESHOLD。AV_SYNC_THRESHOLD 表示视频帧时间差的最大允许值*/
   AV_SYNC_THRESHOLD == 0.01
   sync_threshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;
   /*如果音视频差异的绝对值小于 AV_NOSYNC_THRESHOLD(音视频不同步的阈值)，则根据差异的值调整延迟值。*/
   if (fabs(diff) < AV_NOSYNC_THRESHOLD) {
   /*如果 diff 小于等于负的 sync_threshold，表示视频比音频慢，需要进行延迟调整。将视频延迟归零*/
       if (diff <= -sync_threshold) {
           delay = 0;
       /*视频比音频快*/
       } else if (diff >= sync_threshold) {
           delay = 2 * delay;
       }
   }
   ```

2. 调用视频帧回调函数，将视频帧的数据更新到纹理中。

### 视频帧解封装

```cpp
1.打开媒体文件并初始化 AVFormatContext。
    avformat_open_input() 
2.获取音频和视频流的信息。
	avformat_find_stream_info() 
3.遍历 AVFormatContext 中的流（AVStream），查找视频流和音频流的索引。
4.循环读取数据包（AVPacket），直到文件结束。
	av_read_frame() 
```

### 视频帧解码

```cpp
// 根据视频流id获取编码参数
AVCodecParameters* camCodecParameters = camFmtCtx->streams[videoindex]->codecpar;
// 找到指定的解码器id
AVCodecID codecID = camCodecParameters->codec_id;
// 查找与视频流对应的解码器
const AVCodec* codec = avcodec_find_decoder(codecID);
// 根据解码器分配一个解码上下文
AVCodecContext* camDecodeCtx =avcodec_alloc_context3(codec);
// 将编码参数复制到解码上下文中。
avcodec_parameters_to_context(camDecodeCtx, camCodecParameters);
// 根据解码上下文打开解码器
ret = avcodec_open2(camDecodeCtx, codec, NULL);

上述内容是在解封装的时候准备好的。

//分配AVFrame，两个函数解码，将packet发送到解码器，从解码器读出解码帧
avcodec_send_packet(camDecodeCtx, packetIn);
avcodec_receive_frame(camDecodeCtx, pFrameOut);

将FrameOut转换成RGB的Frame，AVFrame结构体的data[0]就是像素数据
```

## 7.MVC实现的播放器架构？

![Snipaste_2024-07-24_16-56-14](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012016595.png)

为了同学们更好地理解基于MVC架构的音视频播放器，我们将其分为三个主要部分：模型（Model）、视图（View）和控制器（Controller）。

**(1)模型（Model）**

**模型部分负责处理音视频数据的获取、解码和管理。主要组件包括：**

1. **解复用器（Demuxer）：**将输入的音视频流分解成独立的音频流和视频流。
2. **音频解码器（Audio Decoder）：**将压缩的音频数据解码为未压缩的PCM数据。
3. **视频解码器（Video Decoder）：**将压缩的视频数据解码为未压缩的视频帧。
4. **缓冲区（Buffer）：**临时存储解码后的音频数据和视频帧，以保证播放的流畅性。
5. **同步模块（Sync）：**负责音频和视频的同步，以确保音视频同步播放。

**(2)视图（View）**

**视图部分负责展示音视频内容和用户界面，主要组件包括：**

1. 视频显示组件（Video Display Component）：用于渲染解码后的视频帧。
2. 音频输出组件（Audio Output Component）：用于播放解码后的音频数据。
3. 播放控件（Playback Controls）：如播放按钮、暂停按钮、停止按钮、进度条和音量控制等。
4. 用户界面（User Interface）：整体UI布局，包含视频显示区域和播放控件区域。

**(3)控制器（Controller）**

**控制器部分负责处理用户输入，并将这些输入转化为模型的操作，同时更新视图，主要组件包括：**

1. **播放控制器（Playback Controller）：**处理播放、暂停、停止、进度调整和音量控制等操作。
2. **事件处理器（Event Handler）：**接收用户输入（如鼠标点击、拖动进度条等），并调用模型的方法来执行相应操作。

## 8. 音视频播放器的实现技术？

![image-20241001201752629](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012017068.png)

**(1)文件解析与容器格式处理**

1. **解析容器格式：**支持多种音视频容器格式（如MP4、MKV、AVI等），解析文件头信息，提取视频流和音频流。
2. **提取音视频流：**从容器文件中分离音视频流，准备进行解码。

**(2)音视频解码**

1. **选择解码器：**根据音视频格式选择合适的解码器（如H.264、H.265、AAC等）。
2. **解码音视频帧：**利用软件解码（如FFmpeg）或硬件解码器，将压缩的音视频数据解码成未压缩的原始数据。

**(3)同步处理**

1. **时间戳管理：**使用PTS（Presentation Time Stamp）和DTS（Decoding Time Stamp）进行时间管理，确保音视频同步。
2. **同步算法：**基于音频驱动视频或视频驱动音频的方法，调整音视频的播放速度，确保同步。

**(4)音视频渲染**

1. **视频渲染：**将解码后的视频帧传递给图形处理单元（GPU）或直接使用软件渲染，将视频帧显示在屏幕上。
2. **音频渲染：**将解码后的音频数据传递给音频设备，进行播放。

**(5)播放控制**

1. **播放、暂停、停止：**实现基本的播放控制功能，通过控制解码和渲染过程来实现播放、暂停、停止等操作。
2. **快进、快退：**通过调整解码器的读取位置，实现音视频的快进和快退功能。

**(6)错误处理与容错**

1. **错误检测：**检测解码和渲染过程中的错误，如文件损坏、数据丢失等。
2. **容错机制：**实现基本的容错机制，确保播放过程中的平滑和连续性。

**(7)用户界面与交互**

1. **UI设计：**设计用户友好的界面，提供播放控制按钮、进度条、音量调节等功能。
2. **用户交互：**处理用户的交互操作，如点击播放、拖动进度条、调整音量等。

**(8)高级功能（可选）**

1. **字幕支持：**解析和渲染字幕文件（如SRT、ASS等），同步显示字幕。
2. **多音轨、多字幕切换：**支持切换不同音轨和字幕轨道，增强用户体验。
3. **网络流媒体支持：**支持在线流媒体播放，实现RTMP、HLS等流媒体协议的播放功能。

# FFmpeg
## 1.FFmpeg的解复用和解码流程？

```cpp
1.分配空的AVFormatContext
    avformat_alloc_context()
2.打开视频文件填充结构体内容   
	avformat_open_input() 打开视频，将其中的媒体信息填充到AVFormatContext结构体中。
    1.读取数据
    如果输入源是文件，avformat_open_input() 会打开文件并关联 AVIOContext，负责处理文件 I/O。如果输入是网络流，AVIOContext 会负责从网络中读取数据。
    2.用 av_probe_input_format2() 函数来探测封装格式，获得格式信息，得到AVInputFormat结构体，其中包含格式的信息：格式名，探测回调函数，read_header,read_packet回调函数。
    3.根据AVInputFormat结构体中的回调函数，初步解析文件的流，创建AVStream。
3.进一步解析视频码流，获取足够的编解码信息
    用 avformat_find_stream_info() 函数获取流的编解码信息，填充到AVStream内的AVCodecParameters结构体中。
4.确定编解码上下文
    使用 avcodec_find_decoder() 根据 AVCodecParameters结构体中的编码器id，确定解码器的类型，用AVCodec结构体描述。
    根据AVCodec结构体，使用 avcodec_alloc_context3(codec)创建 编解码上下文。使用 avcodec_parameters_to_context(codec_ctx, codec_par)将AVCodecParameters的编解码参数复制到编解码上下文中。
    使用 avcodec_open2()关联编解码器与编解码器上下文，根据编解码器的 init() 函数来初始化编解码器上下文。
5.读取码流内容，放到AVPacket中    
    有了编解码信息， av_read_frame() 函数通过 AVInputFormat 结构体中的 read_packet 函数来读取流的内容，放进AVPacket中。没解码的视频流可能是h264格式，音频流可能是mp3格式。（H264格式涉及到了解析，根据AVCC或者AnnexB获得单帧视频帧）
6.解码
    使用函数 avcodec_send_packet()：将压缩的音视频数据包 (AVPacket) 发送给解码器(h264)，解码器将数据缓存并异步进行解码处理。 avcodec_receive_frame()：从解码器接收解码后的帧 (AVFrame)。
6.帧格式转换
    // 初始化转换上下文，指定源和目标格式、分辨率、缩放算法
    struct SwsContext *sws_ctx = sws_getContext(
        width, height, AV_PIX_FMT_YUV420P,  // 源格式和分辨率
        width, height, AV_PIX_FMT_RGB24,    // 目标格式和分辨率
        SWS_BILINEAR,                       // 使用的缩放算法
        NULL, NULL, NULL
    );
	// 通过 sws_scale 将 YUV420P 转换为 RGB24
    sws_scale(sws_ctx,
         		(const uint8_t * const *)yuv_frame->data, 					yuv_frame->linesize, 0, height,
                rgb_frame->data, rgb_frame->linesize);
7.渲染显示
```

## 2.FFmpeg重要数据结构

![image-20241003170225131](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410032012341.png)

### 0.内存操作 与 初始化

av_malloc:简单的封装了系统函数malloc()，并做了一些错误检查工作。

av_free:封装free()函数。

av_freep:封装了av_free()。并且在释放内存之后将目标指针设置为NULL。

![image-20241003205533652](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410032055583.png)

### 1.AVFormatContext

1. AVFormatContext主要存储音视频**封装**格式中包含的信息；可以分成协议层、格式层、解码层。

   1. `AVIOContext`用来解析输入文件，将数据存储到缓冲区中。
   2. `AVInputFormat`存储输入视音频使用的封装格式。每种视音频封装格式都对应一个`AVInputFormat `结构。
   3. `AVStream`存储一个视频/音频流的相关数据；每个`AVStream`对应一个`AVCodecContext`，存储该视频/音频流使用解码方式的相关数据；每个AVCodecContext中对应一个`AVCodec`，包含该视频/音频对应的解码器。每种解码器都对应一个`AVCodec`结构。

2. 初始化和销毁

   初始化函数是`avformat_alloc_context()`,分配内存，设置`AVFormatContext`字段的默认值。

   ![image-20241003205939074](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410032105107.png)

   销毁函数是`avformat_free_context()`，调用了各式各样的销毁函数：av_opt_free()，av_freep()，av_dict_free()。这些函数分别用于释放不同种类的变量。

```cpp
typedef struct AVFormatContext {
    AVClass *av_class;       // 类信息（日志记录等）
    AVInputFormat *iformat;    // 输入格式指针
    AVOutputFormat *oformat;   // 输出格式指针
    AVIOContext *pb;           // 输入/输出上下文
    unsigned int nb_streams;   // 媒体流数量
    AVStream **streams;       // 流指针数组
    char filename[1024];       // 文件名或 URL
    int64_t start_time;        // 媒体的起始时间
    int64_t duration;         // 媒体文件的时长（微秒）
    int bit_rate;              // 比特率
    AVDictionary *metadata;    // 元数据
    int64_t start_time_realtime;   // 实时流的起始时间
    unsigned int packet_size;     // 数据包大小
    int max_delay;              // 最大延迟
    AVCodec *video_codec;     // 视频解码器指针
    AVCodec *audio_codec;     // 音频解码器指针
    AVCodec *subtitle_codec;  // 字幕解码器指针
    unsigned int flags;       // 标志位
    AVChapter **chapters;     // 章节指针数组
    unsigned int nb_chapters; // 章节数量
    AVProgram **programs;     // 节目指针数组
    unsigned int nb_programs;       // 节目数量
    enum AVCodecID video_codec_id;    // 视频编码器 ID
    enum AVCodecID audio_codec_id;    // 音频编码器 ID
    enum AVCodecID subtitle_codec_id; // 字幕编码器 ID
    AVIOInterruptCB interrupt_callback;// 中断回调
    int max_analyze_duration;         // 最大分析时长
    int64_t probesize;                // 探测大小
    int error_recognition;            // 错误识别标志
    int64_t max_interleave_delta;     // 最大流间隔
    int seek2any;                // 是否允许任意位置跳转
    AVPacketList *packet_buffer; // 缓存未解码的数据包
    
} AVFormatContext;
```

### 2.AVIOContext

AVIOContext的初始化函数是`avio_alloc_context()`，销毁的时候使用`av_free()`释放掉其中的缓存.

`AVIOContext` 是一个**抽象层**，它负责封装底层的 I/O 操作，如文件的 `read` 和 `write` 系统调用，或网络数据的读取和写入。它通过缓冲区机制来存储从底层获取的数据，以便上层的 FFmpeg 函数能够高效地使用这些数据。

AVIOContext中有`URLContext`，每种URL有自己的`URLContext`，URLContext有对应的`URLProtocol`，其中有具体的url_open、url_read等函数来解析具体的网络协议数据

①创建并初始化 `AVIOContext`，并为该文件的 I/O 设置一个内部缓冲区。
②**读取数据到缓冲区**：`AVIOContext` 一次性从磁盘中读取一定大小的数据块到缓冲区中（默认缓冲区大小是 4096 字节，但可以调整）。如果是网络数据，使用read_packet回调函数完成网络数据的读取，该函数与网络协议栈进行交互。
③**从缓冲区读取**：后续的读取操作将从缓冲区中提取数据，而不再直接访问磁盘，直到缓冲区中的数据被消耗殆尽。此时，再从文件系统中读取新的数据块到缓冲区。

```cpp
typedef struct AVIOContext {
    unsigned char *buffer;    // 指向 I/O 缓冲区的指针
    int buffer_size;          // 缓冲区的大小
    unsigned char *buf_ptr; //当前读或写操作指向的缓冲位置
    unsigned char *buf_end; // 缓冲区末尾位置指针
    // 用户定义数据，用于自定义 I/O 上下文，比如内存缓冲区
    void *opaque; // 可以是URLContext结构体
    int (*read_packet)(void *opaque, uint8_t *buf, int buf_size); // 函数指针，定义如何读取数据
    int (*write_packet)(void *opaque, uint8_t *buf, int buf_size); // 函数指针，定义如何写入数据
    int64_t (*seek)(void *opaque, int64_t offset, int whence); // 函数指针，定义如何在数据流中进行 seek 操作
    int64_t pos;         // 当前的文件位置
    int eof_reached;     // 是否到达文件或流的末尾标志位
    int write_flag;      // 标志位，表示是否用于写入操作
    int max_packet_size; // 最大的数据包大小，适用于网络流
    int min_packet_size;
    int64_t bytes_read;  // 累计读取的字节数
    int64_t bytes_written;
    int seekable;        // 指示流是否支持 seek 操作
} AVIOContext;
```

### 3.AVInputFormat

   `AVInputFormat`描述了输入文件的格式信息，`avformat_open_input()` 函数会填充这个结构体。有格式名，探测回调函数，read_header,read_packet回调函数

   ```cpp
typedef struct AVInputFormat {
    const char *name; // 输入格式的短名称
    // 格式的长名称（相对于短名称而言，更易于阅读）
    const char *long_name; 
    /**
        * Can use flags: AVFMT_NOFILE, AVFMT_NEEDNUMBER, AVFMT_SHOW_IDS,AVFMT_GENERIC_INDEX, AVFMT_TS_DISCONT, AVFMT_NOBINSEARCH,AVFMT_NOGENSEARCH,AVFMT_NO_BYTE_SEEK, AVFMT_SEEK_TO_PTS.*/
    int flags // 格式的行为标志，表示如何处理输入流
    // 如果定义了扩展，就不会进行格式探测，不够准确。
    const char *extensions; // 扩展名列表
    // 编码器标记列表
    const struct AVCodecTag * const *codec_tag; 
    //这个类提供了格式特定的选项和日志记录支持。
    const AVClass *priv_class; //用于私有上下文数据 
    // mime类型，它用于在探测时检查匹配的mime类型。
    const char *mime_type; 
    // 探测函数，用于探测文件是否符合此格式。
    int (*read_probe)(AVProbeData *);
    // 读取文件头并初始化 `AVFormatContext`。
    int (*read_header)(struct AVFormatContext *);
    // 读取一个数据包，并存储在提供的 `AVPacket` 结构中。
    int (*read_packet)(struct AVFormatContext *, AVPacket *pkt);
    //实现跳转 (seeking) 功能的函数，通常根据时间戳进行跳转。
    int (*read_seek)(struct AVFormatContext *, int stream_index, int64_t timestamp, int flags);
    // 关闭文件格式并释放任何相关的数据。
    int (*read_close)(struct AVFormatContext *);

    //获取每个流的时间信息，通常在读取几个数据包之后。
    int (*read_timestamp)(struct AVFormatContext *, int stream_index, int64_t *pos, int64_t pos_limit);
    // 初始化流的函数，通常用于手动添加流。
    int (*init_input)(struct AVFormatContext *);
    //释放解复用器使用的私有数据的函数。
    void (*deinit_input)(struct AVFormatContext *);
    //用于返回设备列表（例如视频采集设备）的函数。
    int (*get_device_list)(struct AVFormatContext *, AVDeviceInfoList *);
    //初始化设备（如摄像头、屏幕捕捉设备）功能的函数。
    int (*create_device_capabilities)(struct AVFormatContext *, AVDeviceCapabilitiesQuery *);
    //释放设备功能数据的函数。
    void (*free_device_capabilities)(struct AVFormatContext *, AVDeviceCapabilitiesQuery *);
} AVInputFormat;
   ```

### 4.AVStream

根据格式的AVInputFormat中的`read_header` 从文件的头部读取基础信息，并使用`avformat_new_stream()`创建一个或多个 `AVStream`（音频、视频或字幕流）结构体。每个 `AVStream` 代表文件中的一个流，并包含了该流的编码参数，如编码格式、时长、时间基（time base）等,主要存储流的元数据。

![image-20241003210348247](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410032105659.png)

`read_header` 解析文件时，会识别流的类型和相关的编解码器信息，并将其填充到 `codecpar` 中。这是一个关键步骤，至少会填充 `codec_type`、`codec_id`、`bit_rate` 等基本信息。不过，像 `width`, `height`（视频分辨率）或 `sample_rate`, `channels`（音频采样率、通道数）这样的信息，可能依赖于特定的流类型和文件格式，不一定总是能立即获得。

`avformat_find_stream_info()`函数会读取更多的文件数据，进一步分析并完善每个流的编解码参数。填充AVStream中的AVCodecParameters结构体以及其他内容，比如duration、nb_frames、avg_frame_rate。

AVFormatContext中的`nb_streams` 字段会被更新为流的数量，`streams` 数组则指向所有的 `AVStream` 结构体。

```cpp
typedef struct AVStream {
	AVRational time_base; //视频流的时基。
	/**Decoding解码:流中首帧的显示时间(即流首帧的pts)，单位为上面的时基time_base。
     * 封装上下文AVFormatContext也有一个相同的字段start_time，AVFormatContext中的该字段是由该AVStream的start_time值经过推导后，然后赋值给它的。*/
	int64_t start_time;
	//流的持续时间（以流的time_base为单位）。这表示该流的总时长。
	int64_t duration; 
    int64_t nb_frames;//流中帧的个数。
    AVRational avg_frame_rate;//平均帧率
    AVRational r_frame_rate;//实时帧率。
    /**解复用时不需要我们处理，复用时由我们自定义初始化旧的codec后， 再通过avcodec_parameters_from_context给新的编解码上下文赋值*/
    AVCodecParameters *codecpar;//新版本的编解码上下文。
}AVStream；
```

### 5.AVCodecParameters

存储描述音视频流的参数（如宽度、高度、比特率、采样率、像素格式等），与具体的编解码器实现分离。它通常用于从封装层提取编解码器的静态参数，并用于初始化 `AVCodecContext`。

```cpp
typedef struct AVCodecParameters {
    // 编解码器类型，比如音频、视频、字幕等
    enum AVMediaType codec_type;
    // 编解码器ID，标识特定的编解码器
    enum AVCodecID codec_id;
    // 编解码器的标签 (FourCC 等)
    uint32_t codec_tag;
    // 流的比特率 (以bps为单位)
    int64_t bit_rate;
    // 每个编码样本的比特数
    int bits_per_coded_sample;
    // 每个原始样本的比特数 (用于表示未压缩格式)
    int bits_per_raw_sample;
    // 编解码器的配置文件
    int profile;
    //编解码器的级别（level），用于表示复杂度限制
    int level;
    // 视频流相关成员
    // 视频帧的宽度 (以像素为单位)
    int width;
	// 视频帧的高度 (以像素为单位)
    int height;
	// 视频像素格式
    int format;
 	// 视频帧的采样纵横比（SAR）
    AVRational sample_aspect_ratio;
	//视频帧的场序，指定是逐行扫描还是隔行扫描
    int field_order;

    int sample_rate; // 音频采样率 (如 44100 Hz)
    int channels; // 音频通道数 (如 2 表示立体声)
    //音频通道布局，表示通道的具体配置（如立体声、5.1 声道等）
    uint64_t channel_layout;
    //音频样本格式 (如 AV_SAMPLE_FMT_S16 表示 16 位有符号整数)
    int format;
    // 每帧音频样本包含的采样数
    int frame_size;
    // 额外的编码数据，用于存储编解码器特定的初始化参数
    uint8_t *extradata;
    // 额外编码数据的大小 (以字节为单位)
    int extradata_size;
    // 对齐大小，对于一些编解码器(如 PCM 音频)需要此参数
    int block_align;
    // 初始的音频采样点数
    int initial_padding;
} AVCodecParameters;
```

### 6.AVPacket

AVPacket保存了解复用（demuxer)之后，解码（decode）之前的数据（仍然是压缩后的数据）和关于这些数据的一些附加的信息，如显示时间戳（pts），解码时间戳（dts）,数据时长（duration），所在流媒体的索引（stream_index）等等。

对于视频（Video）来说，AVPacket通常包含一个压缩的Frame；而音频（Audio）则有可能包含多个压缩的Frame。并且，一个packet也有可能是空的，不包含任何压缩数据data，只含有边缘数据side data（side data,容器提供的关于packet的一些附加信息，例如，在编码结束的时候更新一些流的参数）

AVPacket使用`av_packet_alloc`在堆区分配内存，使用`av_packet_free`释放内存。

```cpp
typedef struct AVPacket {
    // 引用计数的缓冲区，用于存储数据包的实际数据
    AVBufferRef *buf;
    int64_t pts; 		// 播放时间
    int64_t dts; 		// 解码时间
    uint8_t *data; 		// 存储已压缩的数据 原始媒体数据
    int   size;    		// 数据大小
    int   stream_index; // 流索引号
    // 包含了一些控制信息，比如该数据包是否是关键帧
    int   flags;
    // 包含了媒体数据包的附加信息，这些信息不会被解码，但可能对于播放或其他处理步骤很重要,比如字幕
    AVPacketSideData *side_data;
    int side_data_elems; //side_data 数组中的元素个数。
    int64_t duration;    // 数据包的播放时长
    int64_t pos;         // 数据包在码流中的字节位置     
    void *opaque;        // 用户的一些私有数据
    AVBufferRef *opaque_ref; // 引用计数缓冲区引用
    AVRational time_base; // 数据包的时间基准,与流相同
} AVPacket;
```

### 7.AVBufferRef

`AVBufferRef` 是一个引用计数管理的结构，有一个指向AVBuffer 结构体。
AVBuffer 是实际存储数据的地方；还包含了一个数据指针 data 和一个缓冲区大小size。AVBuffer中有数据指针data和引用计数器 refcount，以及释放缓冲区的函数指针。

使用 `AVBufferRef` 来管理该 `data` 的引用计数。当你创建多个 `AVBufferRef` 时，它们共享同一个 AVBuffer，但引用计数会增加。

当调用 av_buffer_ref() 时，引用计数增加。
当调用 av_buffer_unref() 时，引用计数减少。当引用计数为零时，AVBuffer 中的数据会被释放。

在使用`av_read_frame`填充AVPacket的时候，AVBufferRef被创建。

AVPacket是否使用AVBufferRef 来管理内存，取决于数据源的类型和解封装器的实现。当从文件读取数据时，`AVPacket` 可能直接管理其数据，而无需 `AVBufferRef`。但在实时流、网络流或设备输入的场景下，`AVBufferRef` 更有可能被使用。

AVFrame也会使用`AVBufferRef `管理数据的内存。

**AVBufferRef优点**：减少内存拷贝、自动释放内存、减少频繁的内存分配释放，减少内存碎片问题

```cpp
typedef struct AVBufferRef {
    AVBuffer *buffer; // 指向实际的缓冲区 AVBuffer 结构
    uint8_t *data;// 指向 AVBuffer的data字段，便于访问
    int size;     // 缓冲区的大小，便于快速获取
} AVBufferRef;

typedef struct AVBuffer {
    uint8_t *data;  // 指向缓冲区的数据指针
    int size;       // 缓冲区的大小
    // 释放缓冲区的函数指针
    void (*free)(void *opaque, uint8_t *data); 
    void *opaque; // 用户自定义数据，用于传递给 free 函数
    int refcount;//引用计数表示当前有多少个引用指向该缓冲区
    void *buffer_private; // 内部使用的私有数据
} AVBuffer;
```

### 8.AVFrame

av_frame_alloc、av_frame_free,在释放结构体之前，先av_frame_unref释放缓存。

重要的数据：

1. data存储视频数据，data[0]存储Y，data[1]存储U，data[2]存储V。RGB使用data[0]存储全部数据。
2. linesize 表示一行像素的字节数。按照 R、G、B 的顺序交织存储。
3. 视频相关：帧宽、帧高、帧的像素格式、pts、dts、帧类型、颜色空间、时间基(单位时间的长度)
4. 音频相关：每个音频通道的采样数、采样格式、采样率、声道数

```cpp
typedef struct AVFrame {
    // 数据指针，存储视频的平面数据或音频的每个声道
    uint8_t *data[AV_NUM_DATA_POINTERS];
    // 每个平面的步幅(即每一行数据的大小)或每个声道的数据大小
    int linesize[AV_NUM_DATA_POINTERS];
    // 存储与 data[] 对应的引用计数缓冲区
    AVBufferRef *buf[AV_NUM_DATA_POINTERS];
    // 视频帧的宽度（像素数）
    int width;
    // 视频帧的高度（像素数）
    int height;
    // 视频像素格式（如 YUV420P, RGB24）
    enum AVPixelFormat format;
    // 音频帧中的样本数
    int nb_samples;
    // 音频的采样率（每秒的样本数）
    int sample_rate;
    // 音频样本格式（如 AV_SAMPLE_FMT_S16，表示16位整型）
    enum AVSampleFormat format;
    // 音频数据的扩展指针，如果声道数超过 AV_NUM_DATA_POINTERS
    uint8_t **extended_data;
    // 音频帧的声道布局（如 AV_CH_LAYOUT_STEREO）
    uint64_t channel_layout;
    // 音频的声道数
    int channels;
    // 帧的显示时间戳，单位由 time_base 决定
    int64_t pts;
    // 解码时间戳
    int64_t pkt_dts;
    // 帧的时间基，用于将时间戳转换为秒数
    AVRational time_base;
    // 帧的颜色空间（如 AVCOL_SPC_BT709）
    enum AVColorSpace colorspace;
    // 帧的颜色范围（如 AVCOL_RANGE_MPEG）
    enum AVColorRange color_range;
    // 帧的元数据，存储键值对形式的额外信息
    AVDictionary *metadata;
    // 帧的标志位，标记帧的特殊属性
    int flags;
    // 视频帧的类型（如 I 帧，P 帧，B 帧）
    enum AVPictureType pict_type;
    // 帧是否是关键帧（1表示关键帧，0表示非关键帧）
    int key_frame;
    // 数据包在输入文件中的字节位置
    int64_t pkt_pos;
    // 重复的帧数，用于帧率控制
    int repeat_pict;
    // 帧是否为隔行扫描
    int interlaced_frame;
    // 表示是否应先显示顶部场
    int top_field_first;
} AVFrame;
```

### 9.AVCodec

在编解码的时候使用，用于确定编解码器

```cpp
typedef struct AVCodec {
    const char *name; // 名字
    const char *long_name;
    enum AVMediaType type; // 类型
    enum AVCodecID id; // id名称
    int capabilities; // 位掩码，功能和能力
    uint8_t max_lowres;                 
    const AVRational *supported_framerates; // 支持的帧率   
    const enum AVPixelFormat *pix_fmts;  // 像素格式   
    const int *supported_samplerates;  // 采样率
    const enum AVSampleFormat *sample_fmts;  // 采样格式
    const AVClass *priv_class;           
    const AVProfile *profiles;//配置
    const char *wrapper_name;
    const AVChannelLayout *ch_layouts;
} AVCodec;
```

### 10.AVCodecContext

**`AVCodecContext`**：专注于编码或解码过程的上下文信息，主要在运行时管理解码器的状态（如帧缓存、解码进度、编解码器配置等）。在实际解码或编码过程中使用。

使用`avcodec_alloc_context3(codec)`根据编码器解码器类型创建编解码上下文，使用`avcodec_parameters_to_context`(codec_ctx, codec_par);将AVCodecParameters的内容复制到编解码上下文中。

```cpp
typedef struct AVCodecContext {
    const AVClass *av_class;  // 类信息
    enum AVMediaType codec_type;  // 编解码器类型
    const struct AVCodec *codec;  // 编解码器
    enum AVCodecID codec_id;  // 编解码器ID
    int64_t bit_rate;  // 平均比特率
    int width, height;  // 视频宽高
    enum AVPixelFormat pix_fmt;  // 像素格式
    AVRational time_base;  // 基准时间
    AVRational framerate;  // 帧率
    int gop_size;  // GOP大小
    int max_b_frames;  // 最大B帧数
    enum AVSampleFormat sample_fmt;  // 音频采样格式
    int sample_rate;  // 音频采样率
    AVChannelLayout ch_layout;  // 音频通道布局
    uint8_t *extradata;  // 额外数据
    int extradata_size;  // 额外数据大小
    int flags;  // 编解码器标志
    int thread_count;  // 线程数
    int (*get_buffer2)(struct AVCodecContext *s, AVFrame *frame, int flags);  // 获取缓冲区回调
    int (*execute)(struct AVCodecContext *c, int (*func)(struct AVCodecContext *c2, void *arg), void *arg2, int *ret, int count, int size);  // 执行多线程任务
    const struct AVHWAccel *hwaccel;  // 硬件加速
    AVBufferRef *hw_frames_ctx;  // 硬件加速帧上下文
    AVBufferRef *hw_device_ctx;  // 硬件设备上下文
} AVCodecContext;
```

## 3.重要的函数

### 1.avformat_open_input()

![image-20241003211919206](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410071857546.png)

0. **分配初始化AVFormatContext**

1. init_input

   从文件名来推测格式，如果得不到，就打开文件，然后根据文件内容进行推测

   **①打开文件 AVIOContext **

   avio_open调用了avio_open2，`avio_open2`调用了2个函数：ffurl_open()和ffio_fdopen()。`ffurl_open()`用于初始化`URLContext`，`ffio_fdopen()`用于根据URLContext初始化`AVIOContext`。URLContext中包含的`URLProtocol`完成了具体的协议读写。URLProtocol中有url_open、read、write、close等函数

   1. ffurl_open()主要调用了2个函数：**ffurl_alloc()**和**ffurl_connect()**。ffurl_alloc()用于查找合适的URLProtocol，并创建一个URLContext；ffurl_connect()用于打开获得的URLProtocol。

      **ffurl_alloc()**两个函数：url_find_protocol()根据文件路径查找合适的URLProtocol，url_alloc_for_protocol()为查找到的URLProtocol创建URLContext。

      **ffurl_connect()**调用URLProtocol的url_open函数。

   2. ffio_fdopen()用于分配内存并初始化`AVIOContext`。AVIOContext中的read_packet、write_packet、seek对应了ffurl_read、write、seek,这三个函数通过retry_transfer_wrapper()函数，将协议中的url_read、write、seek包装，添加错误处理。

   总结：

   如果你没有自定义 `AVIOContext`，那么 `avformat_open_input()` 会使用默认的 `avio_alloc_context()` 来创建一个 `AVIOContext`。这个 `AVIOContext` 会处理与输入 URL 相关的文件操作或者流的读写。如果是本地文件，它将调用操作系统的文件系统 API 来打开文件；如果是网络 URL，则会调用合适的协议处理器（例如 HTTP、RTSP 等）来获取流数据。具体使用`ffurl_open`来调用`ffurl_connect`实际连接到媒体流，针对不同的协议，使用不同的url打开函数。

   **②封装格式探测**

   没有指定封装格式就要探测。

   在`av_probe_input_buffer2()`函数中循环调用avio_read读取数据，再使用`av_probe_input_format2() `函数推测文件格式。这里每次循环会增加读取的数据，一开始是2048字节，二倍扩大，最大在1M左右。

   1. 有一个score变量是判决AVInputFormat的分数的门限值，如果最后得到的AVInputFormat的分数低于该门限值，就认为没有找到合适的AVInputFormat。一般是25分
   2. `av_probe_input_format2()`调用了`av_probe_input_format3()`，当av_probe_input_format3()返回的分数大于门限值的时候，才会返回AVInputFormat。
   3. format3函数中循环调用`av_iformat_next()`遍历FFmpeg中所有的AVInputFormat，并根据以下规则确定AVInputFormat和输入媒体数据的匹配分数
      1. 调用AVInputFormat中的`read_probe()`函数获取匹配分数,如果匹配，得分100.
      2. 没有read_probe()就使用`av_match_ext()`函数比较输入媒体的扩展名和AVInputFormat的扩展名是否匹配，如果匹配得分50
      3. 使用`av_match_name()`比较输入媒体的mime_type和AVInputFormat的mime_type，如果匹配的话75分。

   获得格式信息，得到`AVInputFormat`结构体，其中包含格式的信息：格式名，探测回调函数，read_header,read_packet回调函数。`AVInputFormat`结构体中包含了解复用器信息。

2. **解析视频文件，得到视频流**

   根据`AVInputFormat`结构体中的回调函数` read_header()`，初步解析文件头，创建AVStream。根据`AVInputFormat`中的`read_header` 会从文件的头部读取基础信息，并创建一个或多个 `AVStream`（音频、视频或字幕流）结构体。每个 `AVStream` 代表文件中的一个流，并包含了该流的编码参数，如编码格式、时长、时间基（time base）等。

### 2.avformat_find_stream_info

![image-20241004100126678](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410041001228.png)

find_decoder: 用于找到合适的解码器。调用avcodec_find_decoder获取解码器。

`avformat_find_stream_info()`函数会读取更多的文件数据，进一步分析并完善每个流的编解码参数。

![image-20240928221546165](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410012016886.png)

1. **has_codec_parameters()**编解码信息是否够用了

   在探测流信息时，FFmpeg 需要收集该流的编解码器信息，如编码器类型（视频编码器、音频编码器等）、分辨率、帧率、采样率、声道数等。`has_codec_parameters()` 用来判断这些信息是否完整。如果参数尚未齐全，`avformat_find_stream_info()` 将继续读取更多的数据包，直到能够收集到足够的参数为止。

2. **read_frame_internal()**

   `read_frame_internal()` 是 `av_read_frame()` 的核心，它直接从底层 I/O（`AVIOContext`）中读取数据，并将读取的数据交给解复用器。

3. **try_decode_frame()**

   FFmpeg 可能会尝试解码一些帧，以便确定流的更详细的参数（例如视频的分辨率、帧率，音频的声道数、采样率等）。

   `try_decode_frame()` 会尝试解码一些媒体数据包（`AVPacket`），并通过这些解码后的信息进一步补充流的元数据。(音频、视频、字幕)

   该函数的主要目标是通过解析和解码数据包，探测出那些无法通过简单的文件头信息确定的参数。

解码出来的帧数据并不会保留，会进行释放。av_packet_unref()释放结构体中的资源，av_packet_free()释放结构体本身。 

### 3.avcodec_find_decoder和avcodec_find_encoder

功能：根据codec的id，返回一个AVCodec结构体。

用于确定流对应的编码器、解码器，返回一个AVCodec结构体进行描述。实际其中调用了find_encodec函数。find_encodec函数的第二个参数为0或1(编码)，用于确定找编码器还是解码器，第一个参数是编解码器的ID，比如AV_CODEC_ID_H264.

find_encdec()中有一个循环，该循环会遍历AVCodec结构的链表，逐一比较输入的ID和每一个编码器的ID，直到找到ID取值相等的编码器。其中使用函数`remap_deprecated_codec_id()`将一些过时的编码器ID映射到新的编码器ID。

### 4. avcodec_open2 和 avcodec_close

该函数使用AVCodec来初始化一个视音频编解码器的AVCodecContext.主要使用了AVCodec的init函数，init()是一个函数指针，指向具体编解码器中的初始化函数。

avcodec_close释放AVCodecContext中有关的变量，并且调用了AVCodec的close()关闭了解码器。close()是一个函数指针，指向了特定编码器的关闭函数。

### 5.av_read_fream

读取码流中的音频若干帧或者视频一帧.

![image-20241007185941053](https://cdn.jsdelivr.net/gh/dal-code/imageBed@master/202410071859724.png)

av_read_fream一定能得到一帧数据的AVPacket，一次解析一帧。

1.`avpriv_packet_list_get`从缓冲区中读取AVPacket内容。之前剩下没用的。(这个缓冲区在AVFormatContext中)

2.`read_frame_internal`读取比特流放进AVPacket中。

**ff_read_packet**：调用了AVInputFormat的read_packet()方法，这个函数指针指向对应格式读取数据的函数，将原始字节流根据 封装格式 处理解析然后存入AVPacket中。读字节流数据用到了AVIOContext的读取函数。

**`av_parser_init()`**：初始化用于解析数据包的解析器。解析器会根据不同的解析需求（如帧头解析、完整帧解析等）对数据包进行更细致的处理。

parse_packet->**av_parser_parse2()**：对于h264的码流，根据边界解析成1帧1帧的图像，保存到AVPacket中。(解析与AVCodecParser和AVCodecParserContext相关)

**ff_read_packet**读到的多余的内容保存在FFStream中的parse_queue中，下次解析优先从parse_queue中读取码流。

`avpriv_packet_list_put` 将该 `AVPacket` 存储到 `packet_buffer` 中。这是为了缓存未被立即处理的数据，供后续使用。



